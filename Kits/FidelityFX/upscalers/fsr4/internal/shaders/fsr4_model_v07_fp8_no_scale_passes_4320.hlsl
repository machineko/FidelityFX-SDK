// Copyright(C) 2024-2025 Advanced Micro Devices, Inc. All rights reserved.
// Autogenerated 2025-06-13 11:02:45 using ML2Code from "models\2025\June\pr54\460\fsr4_model_v07_fp8_no_scale.onnx". Do not edit.
// Compile with dxc.exe -no-warnings -O3 -enable-16bit-types -HV 2021 -I hlsl -E fsr4_model_v07_fp8_no_scale_pass0 "generated\dll\fsr4_model_v07_fp8_no_scale.hlsl"

// Scratch memory size needed: 332352256 (317.0MB)

// This file was generated for navi48 SKU
#define WMMA_ENABLED  1
#define FP8_ENABLED  1
#define DOT4_ENABLED  1


ByteAddressBuffer buffer_NHWC_inputs : register(t0);
RWByteAddressBuffer buffer_fused_quantized_NHWC_output : register(u0);
ByteAddressBuffer InitializerBuffer : register(t1);
RWByteAddressBuffer ScratchBuffer : register(u1);

#ifdef MLSR_PASS_0
#define ML2C_GROUPSHARED_SIZE 0

#include "ml2code_runtime/operators/float8_NHWC/Conv2D_k2s2b.hlsli"

[numthreads(32, 1, 1)]
void fsr4_model_v07_fp8_no_scale_pass0(
    uint3 ml2c_dispatchThreadId : SV_DispatchThreadID,
    uint3 ml2c_groupId : SV_GroupID,
    uint3 ml2c_groupThreadId : SV_GroupThreadID
)
{
    const uint3 ml2c_numThreads = uint3(32, 1, 1);
    
    const BufferStorage storage_NHWC_inputs = { buffer_NHWC_inputs };
    const Tensor3h_NHWC< BufferStorage > NHWC_inputs = {
        uint3(7680, 4320, 7), // logicalSize
        uint3(0, 0, 0), // threadGroupSliceStart
        uint3(7680, 4320, 7), // threadGroupSliceSize
        uint3(7680, 4320, 8), // storageSize
        uint3(16, 122880, 2), // storageByteStrides
        uint3(0, 0, 0), // paddingBegin
        uint3(0, 0, 0), // paddingEnd
        0, // threadGroupStorageByteOffset
        storage_NHWC_inputs };
    const BufferStorage storage_encoder1_DownscaleStridedConv2x2_downscale_conv_weight = { InitializerBuffer };
    const Tensor4h_HNWC< BufferStorage > encoder1_DownscaleStridedConv2x2_downscale_conv_weight = {
        uint4(2, 2, 7, 16), // logicalSize
        uint4(0, 0, 0, 0), // threadGroupSliceStart
        uint4(2, 2, 7, 16), // threadGroupSliceSize
        uint4(2, 2, 8, 16), // storageSize
        uint4(16, 512, 2, 32), // storageByteStrides
        uint4(0, 0, 0, 0), // paddingBegin
        uint4(0, 0, 0, 0), // paddingEnd
        0, // threadGroupStorageByteOffset
        storage_encoder1_DownscaleStridedConv2x2_downscale_conv_weight };
    const BufferStorage storage_encoder1_DownscaleStridedConv2x2_downscale_conv_bias = { InitializerBuffer };
    const Tensor1f< BufferStorage > encoder1_DownscaleStridedConv2x2_downscale_conv_bias = {
        16, // logicalSize
        0, // threadGroupSliceStart
        16, // threadGroupSliceSize
        16, // storageSize
        4, // storageByteStrides
        0, // paddingBegin
        0, // paddingEnd
        1024, // threadGroupStorageByteOffset
        storage_encoder1_DownscaleStridedConv2x2_downscale_conv_bias };
    // quantized_NHWC_/encoder2/ResidualBlock_0/body/input_quantization/act_quant/export_handler/QuantizeLinear_output_0
    const uint3 logicalSize_slice_0 = uint3(3840, 2160, 16);
    const int3 groupStart_slice_0 = int3(0, 0, 0) + ml2c_groupId.xyz * int3(16, 1, 16);
    const uint3 groupSize_slice_0 = uint3(16, 1, 16);
    const uint3 storageSize_slice_0 = uint3(3842, 2162, 16);
    const uint3 tensorByteStrides_slice_0 = uint3(16, 61472, 1);
    const uint3 paddingBegin_slice_0 = uint3(1, 1, 0);
    const uint3 paddingEnd_slice_0 = uint3(1, 1, 0);
    const int threadGroupByteOffsetInTensor_slice_0 = dot(groupStart_slice_0, tensorByteStrides_slice_0);
    const float quantizationScale_slice_0 = 1.0;
    const RWBufferStorage storage_slice_0 = { ScratchBuffer };
    const QuantizedTensor3f8_NHWC<RWBufferStorage> slice_0 = { logicalSize_slice_0, groupStart_slice_0, groupSize_slice_0, storageSize_slice_0, tensorByteStrides_slice_0, paddingBegin_slice_0, paddingEnd_slice_0, threadGroupByteOffsetInTensor_slice_0 + 0, quantizationScale_slice_0, storage_slice_0 };
    // /encoder1/DownscaleStridedConv2x2/downscale_conv/Conv (7, 4320, 7680), (16, 7, 2, 2), (16,) -> (16, 2160, 3840)
    ComputeShaderParams computeShaderParams = {ml2c_numThreads, ml2c_groupId, ml2c_groupThreadId, ml2c_dispatchThreadId};
    Conv2D_k2s2b(NHWC_inputs, encoder1_DownscaleStridedConv2x2_downscale_conv_weight, encoder1_DownscaleStridedConv2x2_downscale_conv_bias, slice_0, computeShaderParams);
    
}
#endif // #ifdef MLSR_PASS_0
#ifdef MLSR_PASS_0_POST
#define ML2C_GROUPSHARED_SIZE 0

#include "ml2code_runtime/padding.hlsli"
#include "ml2code_runtime/tensor_float8.hlsli"

[numthreads(32, 1, 1)]
void fsr4_model_v07_fp8_no_scale_pass0_post(
    uint3 ml2c_dispatchThreadId : SV_DispatchThreadID,
    uint3 ml2c_groupId : SV_GroupID
)
{
    // quantized_NHWC_/encoder2/ResidualBlock_0/body/input_quantization/act_quant/export_handler/QuantizeLinear_output_0
    const uint3 logicalSize_slice_1 = uint3(3840, 2160, 16);
    const int3 groupStart_slice_1 = int3(0, 0, 0) + ml2c_groupId.xyz * int3(32, 1, 16);
    const uint3 groupSize_slice_1 = uint3(32, 1, 16);
    const uint3 storageSize_slice_1 = uint3(3842, 2162, 16);
    const uint3 tensorByteStrides_slice_1 = uint3(16, 61472, 1);
    const uint3 paddingBegin_slice_1 = uint3(1, 1, 0);
    const uint3 paddingEnd_slice_1 = uint3(1, 1, 0);
    const int threadGroupByteOffsetInTensor_slice_1 = dot(groupStart_slice_1, tensorByteStrides_slice_1);
    const float quantizationScale_slice_1 = 1.0;
    const RWBufferStorage storage_slice_1 = { ScratchBuffer };
    const QuantizedTensor3f8_NHWC<RWBufferStorage> slice_1 = { logicalSize_slice_1, groupStart_slice_1, groupSize_slice_1, storageSize_slice_1, tensorByteStrides_slice_1, paddingBegin_slice_1, paddingEnd_slice_1, threadGroupByteOffsetInTensor_slice_1 + 0, quantizationScale_slice_1, storage_slice_1 };
    
    StoreDefaultConstBatchOperation < 16, QuantizedTensor3f8_NHWC<RWBufferStorage> > batchOp_0 = { DefaultDword };
    ResetPaddingSeparate(slice_1, ml2c_dispatchThreadId, true, true, batchOp_0, (1, 1));
}
#endif // #ifdef MLSR_PASS_0_POST


#ifdef MLSR_PASS_1
#define ML2C_GROUPSHARED_SIZE 0

#include "ml2code_runtime/operators/float8_NHWC/Fused/ConvNextBlock.hlsli"

[numthreads(32, 1, 1)]
void fsr4_model_v07_fp8_no_scale_pass1(
    uint3 ml2c_dispatchThreadId : SV_DispatchThreadID,
    uint3 ml2c_groupId : SV_GroupID,
    uint3 ml2c_groupThreadId : SV_GroupThreadID
)
{
    const uint3 ml2c_numThreads = uint3(32, 1, 1);
    
    const RWBufferStorage storage_quantized_NHWC__encoder2_ResidualBlock_0_body_input_quantization_act_quant_export_handler_QuantizeLinear_output_0 = { ScratchBuffer };
    const QuantizedTensor3f8_NHWC< RWBufferStorage > quantized_NHWC__encoder2_ResidualBlock_0_body_input_quantization_act_quant_export_handler_QuantizeLinear_output_0 = {
        uint3(3840, 2160, 16), // logicalSize
        uint3(0, 0, 0), // threadGroupSliceStart
        uint3(3840, 2160, 16), // threadGroupSliceSize
        uint3(3842, 2162, 16), // storageSize
        uint3(16, 61472, 1), // storageByteStrides
        uint3(1, 1, 0), // paddingBegin
        uint3(1, 1, 0), // paddingEnd
        0, // threadGroupStorageByteOffset
        1.0, storage_quantized_NHWC__encoder2_ResidualBlock_0_body_input_quantization_act_quant_export_handler_QuantizeLinear_output_0 };
    
    const BufferStorage storage_hwnc__encoder2_ResidualBlock_0_body_conv_dw_weight_quant_export_handler_QuantizeLinear_output_0 = { InitializerBuffer };
    const QuantizedTensor4f8_HWNC< BufferStorage > hwnc__encoder2_ResidualBlock_0_body_conv_dw_weight_quant_export_handler_QuantizeLinear_output_0 = {
        uint4(3, 3, 16, 16), // logicalSize
        uint4(0, 0, 0, 0), // threadGroupSliceStart
        uint4(3, 3, 16, 16), // threadGroupSliceSize
        uint4(3, 3, 16, 16), // storageSize
        uint4(256, 768, 1, 16), // storageByteStrides
        uint4(0, 0, 0, 0), // paddingBegin
        uint4(0, 0, 0, 0), // paddingEnd
        7208, // threadGroupStorageByteOffset
        1.0, storage_hwnc__encoder2_ResidualBlock_0_body_conv_dw_weight_quant_export_handler_QuantizeLinear_output_0 };
    const BufferStorage storage_encoder2_ResidualBlock_0_body_conv_dw_bias = { InitializerBuffer };
    const Tensor1f< BufferStorage > encoder2_ResidualBlock_0_body_conv_dw_bias = {
        16, // logicalSize
        0, // threadGroupSliceStart
        16, // threadGroupSliceSize
        16, // storageSize
        4, // storageByteStrides
        0, // paddingBegin
        0, // paddingEnd
        1088, // threadGroupStorageByteOffset
        storage_encoder2_ResidualBlock_0_body_conv_dw_bias };
    const BufferStorage storage_hwnc__encoder2_ResidualBlock_0_body_conv_pw_expand_weight_quant_export_handler_QuantizeLinear_output_0 = { InitializerBuffer };
    const QuantizedTensor4f8_HWNC< BufferStorage > hwnc__encoder2_ResidualBlock_0_body_conv_pw_expand_weight_quant_export_handler_QuantizeLinear_output_0 = {
        uint4(1, 1, 16, 32), // logicalSize
        uint4(0, 0, 0, 0), // threadGroupSliceStart
        uint4(1, 1, 16, 32), // threadGroupSliceSize
        uint4(1, 1, 16, 32), // storageSize
        uint4(512, 512, 1, 16), // storageByteStrides
        uint4(0, 0, 0, 0), // paddingBegin
        uint4(0, 0, 0, 0), // paddingEnd
        9512, // threadGroupStorageByteOffset
        1.0, storage_hwnc__encoder2_ResidualBlock_0_body_conv_pw_expand_weight_quant_export_handler_QuantizeLinear_output_0 };
    const BufferStorage storage_encoder2_ResidualBlock_0_body_conv_pw_expand_bias = { InitializerBuffer };
    const Tensor1f< BufferStorage > encoder2_ResidualBlock_0_body_conv_pw_expand_bias = {
        32, // logicalSize
        0, // threadGroupSliceStart
        32, // threadGroupSliceSize
        32, // storageSize
        4, // storageByteStrides
        0, // paddingBegin
        0, // paddingEnd
        1152, // threadGroupStorageByteOffset
        storage_encoder2_ResidualBlock_0_body_conv_pw_expand_bias };
    const BufferStorage storage_hwnc__encoder2_ResidualBlock_0_body_conv_pw_contract_weight_quant_export_handler_QuantizeLinear_output_0 = { InitializerBuffer };
    const QuantizedTensor4f8_HWNC< BufferStorage > hwnc__encoder2_ResidualBlock_0_body_conv_pw_contract_weight_quant_export_handler_QuantizeLinear_output_0 = {
        uint4(1, 1, 32, 16), // logicalSize
        uint4(0, 0, 0, 0), // threadGroupSliceStart
        uint4(1, 1, 32, 16), // threadGroupSliceSize
        uint4(1, 1, 32, 16), // storageSize
        uint4(512, 512, 1, 32), // storageByteStrides
        uint4(0, 0, 0, 0), // paddingBegin
        uint4(0, 0, 0, 0), // paddingEnd
        10024, // threadGroupStorageByteOffset
        1.0, storage_hwnc__encoder2_ResidualBlock_0_body_conv_pw_contract_weight_quant_export_handler_QuantizeLinear_output_0 };
    const BufferStorage storage_encoder2_ResidualBlock_0_body_conv_pw_contract_bias = { InitializerBuffer };
    const Tensor1f< BufferStorage > encoder2_ResidualBlock_0_body_conv_pw_contract_bias = {
        16, // logicalSize
        0, // threadGroupSliceStart
        16, // threadGroupSliceSize
        16, // storageSize
        4, // storageByteStrides
        0, // paddingBegin
        0, // paddingEnd
        1280, // threadGroupStorageByteOffset
        storage_encoder2_ResidualBlock_0_body_conv_pw_contract_bias };
    // fused_quantized_NHWC_/encoder2/ResidualBlock_1/body/input_quantization/act_quant/export_handler/QuantizeLinear_output_0
    const uint3 logicalSize_slice_2 = uint3(3840, 2160, 16);
    const int3 groupStart_slice_2 = int3(0, 0, 0) + ml2c_groupId.xyz * int3(16, 8, 16);
    const uint3 groupSize_slice_2 = uint3(16, 8, 16);
    const uint3 storageSize_slice_2 = uint3(3842, 2162, 16);
    const uint3 tensorByteStrides_slice_2 = uint3(16, 61472, 1);
    const uint3 paddingBegin_slice_2 = uint3(1, 1, 0);
    const uint3 paddingEnd_slice_2 = uint3(1, 1, 0);
    const int threadGroupByteOffsetInTensor_slice_2 = dot(groupStart_slice_2, tensorByteStrides_slice_2);
    const float quantizationScale_slice_2 = 1.0;
    const RWBufferStorage storage_slice_2 = { ScratchBuffer };
    const QuantizedTensor3f8_NHWC<RWBufferStorage> slice_2 = { logicalSize_slice_2, groupStart_slice_2, groupSize_slice_2, storageSize_slice_2, tensorByteStrides_slice_2, paddingBegin_slice_2, paddingEnd_slice_2, threadGroupByteOffsetInTensor_slice_2 + 132902464, quantizationScale_slice_2, storage_slice_2 };
    // ConvNextBlock (16, 2160, 3840), (16, 16, 3, 3), (16,), (32, 16, 1, 1), (32,), (16, 32, 1, 1), (16,) -> (16, 2160, 3840)
    ComputeShaderParams computeShaderParams = {ml2c_numThreads, ml2c_groupId, ml2c_groupThreadId, ml2c_dispatchThreadId};
    ConvNextBlock(1.0, 1.0, 1.0, 1.0, quantized_NHWC__encoder2_ResidualBlock_0_body_input_quantization_act_quant_export_handler_QuantizeLinear_output_0, hwnc__encoder2_ResidualBlock_0_body_conv_dw_weight_quant_export_handler_QuantizeLinear_output_0, encoder2_ResidualBlock_0_body_conv_dw_bias, hwnc__encoder2_ResidualBlock_0_body_conv_pw_expand_weight_quant_export_handler_QuantizeLinear_output_0, encoder2_ResidualBlock_0_body_conv_pw_expand_bias, hwnc__encoder2_ResidualBlock_0_body_conv_pw_contract_weight_quant_export_handler_QuantizeLinear_output_0, encoder2_ResidualBlock_0_body_conv_pw_contract_bias, slice_2, computeShaderParams);
    
}
#endif // #ifdef MLSR_PASS_1
#ifdef MLSR_PASS_1_POST
#define ML2C_GROUPSHARED_SIZE 0

#include "ml2code_runtime/padding.hlsli"
#include "ml2code_runtime/tensor_float8.hlsli"

[numthreads(32, 1, 1)]
void fsr4_model_v07_fp8_no_scale_pass1_post(
    uint3 ml2c_dispatchThreadId : SV_DispatchThreadID,
    uint3 ml2c_groupId : SV_GroupID
)
{
    // fused_quantized_NHWC_/encoder2/ResidualBlock_1/body/input_quantization/act_quant/export_handler/QuantizeLinear_output_0
    const uint3 logicalSize_slice_3 = uint3(3840, 2160, 16);
    const int3 groupStart_slice_3 = int3(0, 0, 0) + ml2c_groupId.xyz * int3(32, 1, 16);
    const uint3 groupSize_slice_3 = uint3(32, 1, 16);
    const uint3 storageSize_slice_3 = uint3(3842, 2162, 16);
    const uint3 tensorByteStrides_slice_3 = uint3(16, 61472, 1);
    const uint3 paddingBegin_slice_3 = uint3(1, 1, 0);
    const uint3 paddingEnd_slice_3 = uint3(1, 1, 0);
    const int threadGroupByteOffsetInTensor_slice_3 = dot(groupStart_slice_3, tensorByteStrides_slice_3);
    const float quantizationScale_slice_3 = 1.0;
    const RWBufferStorage storage_slice_3 = { ScratchBuffer };
    const QuantizedTensor3f8_NHWC<RWBufferStorage> slice_3 = { logicalSize_slice_3, groupStart_slice_3, groupSize_slice_3, storageSize_slice_3, tensorByteStrides_slice_3, paddingBegin_slice_3, paddingEnd_slice_3, threadGroupByteOffsetInTensor_slice_3 + 132902464, quantizationScale_slice_3, storage_slice_3 };
    
    StoreDefaultConstBatchOperation < 16, QuantizedTensor3f8_NHWC<RWBufferStorage> > batchOp_0 = { DefaultDword };
    ResetPaddingSeparate(slice_3, ml2c_dispatchThreadId, true, true, batchOp_0, (1, 1));
}
#endif // #ifdef MLSR_PASS_1_POST


#ifdef MLSR_PASS_2
#define ML2C_GROUPSHARED_SIZE 0

#include "ml2code_runtime/operators/float8_NHWC/Fused/ConvNextBlock.hlsli"

[numthreads(32, 1, 1)]
void fsr4_model_v07_fp8_no_scale_pass2(
    uint3 ml2c_dispatchThreadId : SV_DispatchThreadID,
    uint3 ml2c_groupId : SV_GroupID,
    uint3 ml2c_groupThreadId : SV_GroupThreadID
)
{
    const uint3 ml2c_numThreads = uint3(32, 1, 1);
    
    const RWBufferStorage storage_fused_quantized_NHWC__encoder2_ResidualBlock_1_body_input_quantization_act_quant_export_handler_QuantizeLinear_output_0 = { ScratchBuffer };
    const QuantizedTensor3f8_NHWC< RWBufferStorage > fused_quantized_NHWC__encoder2_ResidualBlock_1_body_input_quantization_act_quant_export_handler_QuantizeLinear_output_0 = {
        uint3(3840, 2160, 16), // logicalSize
        uint3(0, 0, 0), // threadGroupSliceStart
        uint3(3840, 2160, 16), // threadGroupSliceSize
        uint3(3842, 2162, 16), // storageSize
        uint3(16, 61472, 1), // storageByteStrides
        uint3(1, 1, 0), // paddingBegin
        uint3(1, 1, 0), // paddingEnd
        132902464, // threadGroupStorageByteOffset
        1.0, storage_fused_quantized_NHWC__encoder2_ResidualBlock_1_body_input_quantization_act_quant_export_handler_QuantizeLinear_output_0 };
    
    const BufferStorage storage_hwnc__encoder2_ResidualBlock_1_body_conv_dw_weight_quant_export_handler_QuantizeLinear_output_0 = { InitializerBuffer };
    const QuantizedTensor4f8_HWNC< BufferStorage > hwnc__encoder2_ResidualBlock_1_body_conv_dw_weight_quant_export_handler_QuantizeLinear_output_0 = {
        uint4(3, 3, 16, 16), // logicalSize
        uint4(0, 0, 0, 0), // threadGroupSliceStart
        uint4(3, 3, 16, 16), // threadGroupSliceSize
        uint4(3, 3, 16, 16), // storageSize
        uint4(256, 768, 1, 16), // storageByteStrides
        uint4(0, 0, 0, 0), // paddingBegin
        uint4(0, 0, 0, 0), // paddingEnd
        10536, // threadGroupStorageByteOffset
        1.0, storage_hwnc__encoder2_ResidualBlock_1_body_conv_dw_weight_quant_export_handler_QuantizeLinear_output_0 };
    const BufferStorage storage_encoder2_ResidualBlock_1_body_conv_dw_bias = { InitializerBuffer };
    const Tensor1f< BufferStorage > encoder2_ResidualBlock_1_body_conv_dw_bias = {
        16, // logicalSize
        0, // threadGroupSliceStart
        16, // threadGroupSliceSize
        16, // storageSize
        4, // storageByteStrides
        0, // paddingBegin
        0, // paddingEnd
        1344, // threadGroupStorageByteOffset
        storage_encoder2_ResidualBlock_1_body_conv_dw_bias };
    const BufferStorage storage_hwnc__encoder2_ResidualBlock_1_body_conv_pw_expand_weight_quant_export_handler_QuantizeLinear_output_0 = { InitializerBuffer };
    const QuantizedTensor4f8_HWNC< BufferStorage > hwnc__encoder2_ResidualBlock_1_body_conv_pw_expand_weight_quant_export_handler_QuantizeLinear_output_0 = {
        uint4(1, 1, 16, 32), // logicalSize
        uint4(0, 0, 0, 0), // threadGroupSliceStart
        uint4(1, 1, 16, 32), // threadGroupSliceSize
        uint4(1, 1, 16, 32), // storageSize
        uint4(512, 512, 1, 16), // storageByteStrides
        uint4(0, 0, 0, 0), // paddingBegin
        uint4(0, 0, 0, 0), // paddingEnd
        12840, // threadGroupStorageByteOffset
        1.0, storage_hwnc__encoder2_ResidualBlock_1_body_conv_pw_expand_weight_quant_export_handler_QuantizeLinear_output_0 };
    const BufferStorage storage_encoder2_ResidualBlock_1_body_conv_pw_expand_bias = { InitializerBuffer };
    const Tensor1f< BufferStorage > encoder2_ResidualBlock_1_body_conv_pw_expand_bias = {
        32, // logicalSize
        0, // threadGroupSliceStart
        32, // threadGroupSliceSize
        32, // storageSize
        4, // storageByteStrides
        0, // paddingBegin
        0, // paddingEnd
        1408, // threadGroupStorageByteOffset
        storage_encoder2_ResidualBlock_1_body_conv_pw_expand_bias };
    const BufferStorage storage_hwnc__encoder2_ResidualBlock_1_body_conv_pw_contract_weight_quant_export_handler_QuantizeLinear_output_0 = { InitializerBuffer };
    const QuantizedTensor4f8_HWNC< BufferStorage > hwnc__encoder2_ResidualBlock_1_body_conv_pw_contract_weight_quant_export_handler_QuantizeLinear_output_0 = {
        uint4(1, 1, 32, 16), // logicalSize
        uint4(0, 0, 0, 0), // threadGroupSliceStart
        uint4(1, 1, 32, 16), // threadGroupSliceSize
        uint4(1, 1, 32, 16), // storageSize
        uint4(512, 512, 1, 32), // storageByteStrides
        uint4(0, 0, 0, 0), // paddingBegin
        uint4(0, 0, 0, 0), // paddingEnd
        13352, // threadGroupStorageByteOffset
        1.0, storage_hwnc__encoder2_ResidualBlock_1_body_conv_pw_contract_weight_quant_export_handler_QuantizeLinear_output_0 };
    const BufferStorage storage_encoder2_ResidualBlock_1_body_conv_pw_contract_bias = { InitializerBuffer };
    const Tensor1f< BufferStorage > encoder2_ResidualBlock_1_body_conv_pw_contract_bias = {
        16, // logicalSize
        0, // threadGroupSliceStart
        16, // threadGroupSliceSize
        16, // storageSize
        4, // storageByteStrides
        0, // paddingBegin
        0, // paddingEnd
        1536, // threadGroupStorageByteOffset
        storage_encoder2_ResidualBlock_1_body_conv_pw_contract_bias };
    // fused_quantized_NHWC_/encoder2/DownscaleStridedConv2x2/skip_func/act_quant/export_handler/QuantizeLinear_output_0
    const uint3 logicalSize_slice_4 = uint3(3840, 2160, 16);
    const int3 groupStart_slice_4 = int3(0, 0, 0) + ml2c_groupId.xyz * int3(16, 8, 16);
    const uint3 groupSize_slice_4 = uint3(16, 8, 16);
    const uint3 storageSize_slice_4 = uint3(3842, 2162, 16);
    const uint3 tensorByteStrides_slice_4 = uint3(16, 61472, 1);
    const uint3 paddingBegin_slice_4 = uint3(1, 1, 0);
    const uint3 paddingEnd_slice_4 = uint3(1, 1, 0);
    const int threadGroupByteOffsetInTensor_slice_4 = dot(groupStart_slice_4, tensorByteStrides_slice_4);
    const float quantizationScale_slice_4 = 1.0;
    const RWBufferStorage storage_slice_4 = { ScratchBuffer };
    const QuantizedTensor3f8_NHWC<RWBufferStorage> slice_4 = { logicalSize_slice_4, groupStart_slice_4, groupSize_slice_4, storageSize_slice_4, tensorByteStrides_slice_4, paddingBegin_slice_4, paddingEnd_slice_4, threadGroupByteOffsetInTensor_slice_4 + 0, quantizationScale_slice_4, storage_slice_4 };
    // ConvNextBlock (16, 2160, 3840), (16, 16, 3, 3), (16,), (32, 16, 1, 1), (32,), (16, 32, 1, 1), (16,) -> (16, 2160, 3840)
    ComputeShaderParams computeShaderParams = {ml2c_numThreads, ml2c_groupId, ml2c_groupThreadId, ml2c_dispatchThreadId};
    ConvNextBlock(1.0, 1.0, 1.0, 1.0, fused_quantized_NHWC__encoder2_ResidualBlock_1_body_input_quantization_act_quant_export_handler_QuantizeLinear_output_0, hwnc__encoder2_ResidualBlock_1_body_conv_dw_weight_quant_export_handler_QuantizeLinear_output_0, encoder2_ResidualBlock_1_body_conv_dw_bias, hwnc__encoder2_ResidualBlock_1_body_conv_pw_expand_weight_quant_export_handler_QuantizeLinear_output_0, encoder2_ResidualBlock_1_body_conv_pw_expand_bias, hwnc__encoder2_ResidualBlock_1_body_conv_pw_contract_weight_quant_export_handler_QuantizeLinear_output_0, encoder2_ResidualBlock_1_body_conv_pw_contract_bias, slice_4, computeShaderParams);
    
}
#endif // #ifdef MLSR_PASS_2
#ifdef MLSR_PASS_2_POST
#define ML2C_GROUPSHARED_SIZE 0

#include "ml2code_runtime/padding.hlsli"
#include "ml2code_runtime/tensor_float8.hlsli"

[numthreads(32, 1, 1)]
void fsr4_model_v07_fp8_no_scale_pass2_post(
    uint3 ml2c_dispatchThreadId : SV_DispatchThreadID,
    uint3 ml2c_groupId : SV_GroupID
)
{
    // fused_quantized_NHWC_/encoder2/DownscaleStridedConv2x2/skip_func/act_quant/export_handler/QuantizeLinear_output_0
    const uint3 logicalSize_slice_5 = uint3(3840, 2160, 16);
    const int3 groupStart_slice_5 = int3(0, 0, 0) + ml2c_groupId.xyz * int3(32, 1, 16);
    const uint3 groupSize_slice_5 = uint3(32, 1, 16);
    const uint3 storageSize_slice_5 = uint3(3842, 2162, 16);
    const uint3 tensorByteStrides_slice_5 = uint3(16, 61472, 1);
    const uint3 paddingBegin_slice_5 = uint3(1, 1, 0);
    const uint3 paddingEnd_slice_5 = uint3(1, 1, 0);
    const int threadGroupByteOffsetInTensor_slice_5 = dot(groupStart_slice_5, tensorByteStrides_slice_5);
    const float quantizationScale_slice_5 = 1.0;
    const RWBufferStorage storage_slice_5 = { ScratchBuffer };
    const QuantizedTensor3f8_NHWC<RWBufferStorage> slice_5 = { logicalSize_slice_5, groupStart_slice_5, groupSize_slice_5, storageSize_slice_5, tensorByteStrides_slice_5, paddingBegin_slice_5, paddingEnd_slice_5, threadGroupByteOffsetInTensor_slice_5 + 0, quantizationScale_slice_5, storage_slice_5 };
    
    StoreDefaultConstBatchOperation < 16, QuantizedTensor3f8_NHWC<RWBufferStorage> > batchOp_0 = { DefaultDword };
    ResetPaddingSeparate(slice_5, ml2c_dispatchThreadId, true, true, batchOp_0, (1, 1));
}
#endif // #ifdef MLSR_PASS_2_POST


#ifdef MLSR_PASS_3
#define ML2C_GROUPSHARED_SIZE 0

#include "ml2code_runtime/operators/float8_NHWC/Fused/FusedConv2D_k2s2b_QuantizedOutput.hlsli"

[numthreads(32, 1, 1)]
void fsr4_model_v07_fp8_no_scale_pass3(
    uint3 ml2c_dispatchThreadId : SV_DispatchThreadID,
    uint3 ml2c_groupId : SV_GroupID,
    uint3 ml2c_groupThreadId : SV_GroupThreadID
)
{
    const uint3 ml2c_numThreads = uint3(32, 1, 1);
    
    const RWBufferStorage storage_fused_quantized_NHWC__encoder2_DownscaleStridedConv2x2_skip_func_act_quant_export_handler_QuantizeLinear_output_0 = { ScratchBuffer };
    const QuantizedTensor3f8_NHWC< RWBufferStorage > fused_quantized_NHWC__encoder2_DownscaleStridedConv2x2_skip_func_act_quant_export_handler_QuantizeLinear_output_0 = {
        uint3(3840, 2160, 16), // logicalSize
        uint3(0, 0, 0), // threadGroupSliceStart
        uint3(3840, 2160, 16), // threadGroupSliceSize
        uint3(3842, 2162, 16), // storageSize
        uint3(16, 61472, 1), // storageByteStrides
        uint3(1, 1, 0), // paddingBegin
        uint3(1, 1, 0), // paddingEnd
        0, // threadGroupStorageByteOffset
        1.0, storage_fused_quantized_NHWC__encoder2_DownscaleStridedConv2x2_skip_func_act_quant_export_handler_QuantizeLinear_output_0 };
    
    const BufferStorage storage_hwnc__encoder2_DownscaleStridedConv2x2_downscale_conv_weight_quant_export_handler_QuantizeLinear_output_0 = { InitializerBuffer };
    const QuantizedTensor4f8_HWNC< BufferStorage > hwnc__encoder2_DownscaleStridedConv2x2_downscale_conv_weight_quant_export_handler_QuantizeLinear_output_0 = {
        uint4(2, 2, 16, 32), // logicalSize
        uint4(0, 0, 0, 0), // threadGroupSliceStart
        uint4(2, 2, 16, 32), // threadGroupSliceSize
        uint4(2, 2, 16, 32), // storageSize
        uint4(512, 1024, 1, 16), // storageByteStrides
        uint4(0, 0, 0, 0), // paddingBegin
        uint4(0, 0, 0, 0), // paddingEnd
        13864, // threadGroupStorageByteOffset
        1.0, storage_hwnc__encoder2_DownscaleStridedConv2x2_downscale_conv_weight_quant_export_handler_QuantizeLinear_output_0 };
    const BufferStorage storage_encoder2_DownscaleStridedConv2x2_downscale_conv_bias = { InitializerBuffer };
    const Tensor1f< BufferStorage > encoder2_DownscaleStridedConv2x2_downscale_conv_bias = {
        32, // logicalSize
        0, // threadGroupSliceStart
        32, // threadGroupSliceSize
        32, // storageSize
        4, // storageByteStrides
        0, // paddingBegin
        0, // paddingEnd
        1600, // threadGroupStorageByteOffset
        storage_encoder2_DownscaleStridedConv2x2_downscale_conv_bias };
    // Fusedquantized_/encoder2/DownscaleStridedConv2x2/downscale_conv/Conv_quantized_outputs_output_grouped
    const uint3 logicalSize_slice_6 = uint3(1920, 1080, 32);
    const int3 groupStart_slice_6 = int3(0, 0, 0) + ml2c_groupId.xyz * int3(32, 1, 32);
    const uint3 groupSize_slice_6 = uint3(32, 1, 32);
    const uint3 storageSize_slice_6 = uint3(1922, 1082, 32);
    const uint3 tensorByteStrides_slice_6 = uint3(32, 61504, 1);
    const uint3 paddingBegin_slice_6 = uint3(1, 1, 0);
    const uint3 paddingEnd_slice_6 = uint3(1, 1, 0);
    const int threadGroupByteOffsetInTensor_slice_6 = dot(groupStart_slice_6, tensorByteStrides_slice_6);
    const RWBufferStorage storage_slice_6 = { ScratchBuffer };
    const Tensor3f8_NHWC<RWBufferStorage> slice_6 = { logicalSize_slice_6, groupStart_slice_6, groupSize_slice_6, storageSize_slice_6, tensorByteStrides_slice_6, paddingBegin_slice_6, paddingEnd_slice_6, threadGroupByteOffsetInTensor_slice_6 + 132902464, storage_slice_6 };
    // Fusedquantized_/encoder2/DownscaleStridedConv2x2/downscale_conv/Conv_quantized_outputs (16, 2160, 3840), (32, 16, 2, 2), (32,) -> (32, 1080, 1920)
    ComputeShaderParams computeShaderParams = {ml2c_numThreads, ml2c_groupId, ml2c_groupThreadId, ml2c_dispatchThreadId};
    FusedConv2D_k2s2b_QuantizedOutput(1.0, 1.0, fused_quantized_NHWC__encoder2_DownscaleStridedConv2x2_skip_func_act_quant_export_handler_QuantizeLinear_output_0, hwnc__encoder2_DownscaleStridedConv2x2_downscale_conv_weight_quant_export_handler_QuantizeLinear_output_0, encoder2_DownscaleStridedConv2x2_downscale_conv_bias, slice_6, computeShaderParams);
    
}
#endif // #ifdef MLSR_PASS_3
#ifdef MLSR_PASS_3_POST
#define ML2C_GROUPSHARED_SIZE 0

#include "ml2code_runtime/padding.hlsli"
#include "ml2code_runtime/tensor_float8.hlsli"

[numthreads(32, 1, 1)]
void fsr4_model_v07_fp8_no_scale_pass3_post(
    uint3 ml2c_dispatchThreadId : SV_DispatchThreadID,
    uint3 ml2c_groupId : SV_GroupID
)
{
    // Fusedquantized_/encoder2/DownscaleStridedConv2x2/downscale_conv/Conv_quantized_outputs_output_grouped
    const uint3 logicalSize_slice_7 = uint3(1920, 1080, 32);
    const int3 groupStart_slice_7 = int3(0, 0, 0) + ml2c_groupId.xyz * int3(32, 1, 32);
    const uint3 groupSize_slice_7 = uint3(32, 1, 32);
    const uint3 storageSize_slice_7 = uint3(1922, 1082, 32);
    const uint3 tensorByteStrides_slice_7 = uint3(32, 61504, 1);
    const uint3 paddingBegin_slice_7 = uint3(1, 1, 0);
    const uint3 paddingEnd_slice_7 = uint3(1, 1, 0);
    const int threadGroupByteOffsetInTensor_slice_7 = dot(groupStart_slice_7, tensorByteStrides_slice_7);
    const RWBufferStorage storage_slice_7 = { ScratchBuffer };
    const Tensor3f8_NHWC<RWBufferStorage> slice_7 = { logicalSize_slice_7, groupStart_slice_7, groupSize_slice_7, storageSize_slice_7, tensorByteStrides_slice_7, paddingBegin_slice_7, paddingEnd_slice_7, threadGroupByteOffsetInTensor_slice_7 + 132902464, storage_slice_7 };
    
    StoreDefaultConstBatchOperation < 16, Tensor3f8_NHWC<RWBufferStorage> > batchOp_0 = { DefaultDword };
    ResetPaddingSeparate(slice_7, ml2c_dispatchThreadId, true, true, batchOp_0, (1, 1));
}
#endif // #ifdef MLSR_PASS_3_POST


#ifdef MLSR_PASS_4
#define ML2C_GROUPSHARED_SIZE 0

#include "ml2code_runtime/operators/float8_NHWC/Fused/FasterNetBlock.hlsli"

[numthreads(32, 1, 1)]
void fsr4_model_v07_fp8_no_scale_pass4(
    uint3 ml2c_dispatchThreadId : SV_DispatchThreadID,
    uint3 ml2c_groupId : SV_GroupID,
    uint3 ml2c_groupThreadId : SV_GroupThreadID
)
{
    const uint3 ml2c_numThreads = uint3(32, 1, 1);
    
    const RWBufferStorage storage_Fusedquantized__encoder2_DownscaleStridedConv2x2_downscale_conv_Conv_quantized_outputs_output_grouped = { ScratchBuffer };
    const Tensor3f8_NHWC< RWBufferStorage > Fusedquantized__encoder2_DownscaleStridedConv2x2_downscale_conv_Conv_quantized_outputs_output_grouped = {
        uint3(1920, 1080, 32), // logicalSize
        uint3(0, 0, 0), // threadGroupSliceStart
        uint3(1920, 1080, 32), // threadGroupSliceSize
        uint3(1922, 1082, 32), // storageSize
        uint3(32, 61504, 1), // storageByteStrides
        uint3(1, 1, 0), // paddingBegin
        uint3(1, 1, 0), // paddingEnd
        132902464, // threadGroupStorageByteOffset
        storage_Fusedquantized__encoder2_DownscaleStridedConv2x2_downscale_conv_Conv_quantized_outputs_output_grouped };
    
    const BufferStorage storage_hwnc__encoder3_ResidualBlock_0_body_spatial_mixing_partial_conv_weight_quant_export_handler_QuantizeLinear_output_0 = { InitializerBuffer };
    const QuantizedTensor4f8_HWNC< BufferStorage > hwnc__encoder3_ResidualBlock_0_body_spatial_mixing_partial_conv_weight_quant_export_handler_QuantizeLinear_output_0 = {
        uint4(3, 3, 16, 16), // logicalSize
        uint4(0, 0, 0, 0), // threadGroupSliceStart
        uint4(3, 3, 16, 16), // threadGroupSliceSize
        uint4(3, 3, 16, 16), // storageSize
        uint4(256, 768, 1, 16), // storageByteStrides
        uint4(0, 0, 0, 0), // paddingBegin
        uint4(0, 0, 0, 0), // paddingEnd
        15912, // threadGroupStorageByteOffset
        1.0, storage_hwnc__encoder3_ResidualBlock_0_body_spatial_mixing_partial_conv_weight_quant_export_handler_QuantizeLinear_output_0 };
    const BufferStorage storage_encoder3_ResidualBlock_0_body_spatial_mixing_partial_conv_bias = { InitializerBuffer };
    const Tensor1f< BufferStorage > encoder3_ResidualBlock_0_body_spatial_mixing_partial_conv_bias = {
        16, // logicalSize
        0, // threadGroupSliceStart
        16, // threadGroupSliceSize
        16, // storageSize
        4, // storageByteStrides
        0, // paddingBegin
        0, // paddingEnd
        1728, // threadGroupStorageByteOffset
        storage_encoder3_ResidualBlock_0_body_spatial_mixing_partial_conv_bias };
    const BufferStorage storage_hwnc__encoder3_ResidualBlock_0_body_pw_expand_weight_quant_export_handler_QuantizeLinear_output_0 = { InitializerBuffer };
    const QuantizedTensor4f8_HWNC< BufferStorage > hwnc__encoder3_ResidualBlock_0_body_pw_expand_weight_quant_export_handler_QuantizeLinear_output_0 = {
        uint4(1, 1, 32, 64), // logicalSize
        uint4(0, 0, 0, 0), // threadGroupSliceStart
        uint4(1, 1, 32, 64), // threadGroupSliceSize
        uint4(1, 1, 32, 64), // storageSize
        uint4(2048, 2048, 1, 32), // storageByteStrides
        uint4(0, 0, 0, 0), // paddingBegin
        uint4(0, 0, 0, 0), // paddingEnd
        18216, // threadGroupStorageByteOffset
        1.0, storage_hwnc__encoder3_ResidualBlock_0_body_pw_expand_weight_quant_export_handler_QuantizeLinear_output_0 };
    const BufferStorage storage_encoder3_ResidualBlock_0_body_pw_expand_bias = { InitializerBuffer };
    const Tensor1f< BufferStorage > encoder3_ResidualBlock_0_body_pw_expand_bias = {
        64, // logicalSize
        0, // threadGroupSliceStart
        64, // threadGroupSliceSize
        64, // storageSize
        4, // storageByteStrides
        0, // paddingBegin
        0, // paddingEnd
        1792, // threadGroupStorageByteOffset
        storage_encoder3_ResidualBlock_0_body_pw_expand_bias };
    const BufferStorage storage_hwnc__encoder3_ResidualBlock_0_body_pw_contract_weight_quant_export_handler_QuantizeLinear_output_0 = { InitializerBuffer };
    const QuantizedTensor4f8_HWNC< BufferStorage > hwnc__encoder3_ResidualBlock_0_body_pw_contract_weight_quant_export_handler_QuantizeLinear_output_0 = {
        uint4(1, 1, 64, 32), // logicalSize
        uint4(0, 0, 0, 0), // threadGroupSliceStart
        uint4(1, 1, 64, 32), // threadGroupSliceSize
        uint4(1, 1, 64, 32), // storageSize
        uint4(2048, 2048, 1, 64), // storageByteStrides
        uint4(0, 0, 0, 0), // paddingBegin
        uint4(0, 0, 0, 0), // paddingEnd
        20264, // threadGroupStorageByteOffset
        1.0, storage_hwnc__encoder3_ResidualBlock_0_body_pw_contract_weight_quant_export_handler_QuantizeLinear_output_0 };
    const BufferStorage storage_encoder3_ResidualBlock_0_body_pw_contract_bias = { InitializerBuffer };
    const Tensor1f< BufferStorage > encoder3_ResidualBlock_0_body_pw_contract_bias = {
        32, // logicalSize
        0, // threadGroupSliceStart
        32, // threadGroupSliceSize
        32, // storageSize
        4, // storageByteStrides
        0, // paddingBegin
        0, // paddingEnd
        2048, // threadGroupStorageByteOffset
        storage_encoder3_ResidualBlock_0_body_pw_contract_bias };
    // /encoder3/ResidualBlock_1/residual_func/Split_output_grouped
    const uint3 logicalSize_slice_8 = uint3(1920, 1080, 32);
    const int3 groupStart_slice_8 = int3(0, 0, 0) + ml2c_groupId.xyz * int3(32, 4, 32);
    const uint3 groupSize_slice_8 = uint3(32, 4, 32);
    const uint3 storageSize_slice_8 = uint3(1922, 1082, 32);
    const uint3 tensorByteStrides_slice_8 = uint3(32, 61504, 1);
    const uint3 paddingBegin_slice_8 = uint3(1, 1, 0);
    const uint3 paddingEnd_slice_8 = uint3(1, 1, 0);
    const int threadGroupByteOffsetInTensor_slice_8 = dot(groupStart_slice_8, tensorByteStrides_slice_8);
    const RWBufferStorage storage_slice_8 = { ScratchBuffer };
    const Tensor3f8_NHWC<RWBufferStorage> slice_8 = { logicalSize_slice_8, groupStart_slice_8, groupSize_slice_8, storageSize_slice_8, tensorByteStrides_slice_8, paddingBegin_slice_8, paddingEnd_slice_8, threadGroupByteOffsetInTensor_slice_8 + 265804928, storage_slice_8 };
    // Fusedquantized_/encoder3/ResidualBlock_0/residual_func/Concat_quantized_/encoder3/ResidualBlock_0/body/spatial_mixing/partial_conv/Conv_quantized_/encoder3/ResidualBlock_0/body/spatial_mixing/Concat_quantized_/encoder3/ResidualBlock_0/body/pw_expand/Conv_/encoder3/ResidualBlock_0/body/pw_expand_act/Relu_quantized_/encoder3/ResidualBlock_0/body/pw_contract/Conv_/encoder3/ResidualBlock_0/Add (32, 1080, 1920), (16, 16, 3, 3), (16,), (64, 32, 1, 1), (64,), (32, 64, 1, 1), (32,) -> (32, 1080, 1920)
    ComputeShaderParams computeShaderParams = {ml2c_numThreads, ml2c_groupId, ml2c_groupThreadId, ml2c_dispatchThreadId};
    FasterNetBlock<32, 1>(1.0, 1.0, 1.0, 1.0, 1.0, Fusedquantized__encoder2_DownscaleStridedConv2x2_downscale_conv_Conv_quantized_outputs_output_grouped, hwnc__encoder3_ResidualBlock_0_body_spatial_mixing_partial_conv_weight_quant_export_handler_QuantizeLinear_output_0, encoder3_ResidualBlock_0_body_spatial_mixing_partial_conv_bias, hwnc__encoder3_ResidualBlock_0_body_pw_expand_weight_quant_export_handler_QuantizeLinear_output_0, encoder3_ResidualBlock_0_body_pw_expand_bias, hwnc__encoder3_ResidualBlock_0_body_pw_contract_weight_quant_export_handler_QuantizeLinear_output_0, encoder3_ResidualBlock_0_body_pw_contract_bias, slice_8, computeShaderParams);
    
}
#endif // #ifdef MLSR_PASS_4
#ifdef MLSR_PASS_4_POST
#define ML2C_GROUPSHARED_SIZE 0

#include "ml2code_runtime/padding.hlsli"
#include "ml2code_runtime/tensor_float8.hlsli"

[numthreads(32, 1, 1)]
void fsr4_model_v07_fp8_no_scale_pass4_post(
    uint3 ml2c_dispatchThreadId : SV_DispatchThreadID,
    uint3 ml2c_groupId : SV_GroupID
)
{
    // /encoder3/ResidualBlock_1/residual_func/Split_output_grouped
    const uint3 logicalSize_slice_9 = uint3(1920, 1080, 32);
    const int3 groupStart_slice_9 = int3(0, 0, 0) + ml2c_groupId.xyz * int3(32, 1, 32);
    const uint3 groupSize_slice_9 = uint3(32, 1, 32);
    const uint3 storageSize_slice_9 = uint3(1922, 1082, 32);
    const uint3 tensorByteStrides_slice_9 = uint3(32, 61504, 1);
    const uint3 paddingBegin_slice_9 = uint3(1, 1, 0);
    const uint3 paddingEnd_slice_9 = uint3(1, 1, 0);
    const int threadGroupByteOffsetInTensor_slice_9 = dot(groupStart_slice_9, tensorByteStrides_slice_9);
    const RWBufferStorage storage_slice_9 = { ScratchBuffer };
    const Tensor3f8_NHWC<RWBufferStorage> slice_9 = { logicalSize_slice_9, groupStart_slice_9, groupSize_slice_9, storageSize_slice_9, tensorByteStrides_slice_9, paddingBegin_slice_9, paddingEnd_slice_9, threadGroupByteOffsetInTensor_slice_9 + 265804928, storage_slice_9 };
    
    StoreDefaultConstBatchOperation < 16, Tensor3f8_NHWC<RWBufferStorage> > batchOp_0 = { DefaultDword };
    ResetPaddingSeparate(slice_9, ml2c_dispatchThreadId, true, true, batchOp_0, (1, 1));
}
#endif // #ifdef MLSR_PASS_4_POST


#ifdef MLSR_PASS_5
#define ML2C_GROUPSHARED_SIZE 0

#include "ml2code_runtime/operators/float8_NHWC/Fused/FasterNetBlock.hlsli"

[numthreads(32, 1, 1)]
void fsr4_model_v07_fp8_no_scale_pass5(
    uint3 ml2c_dispatchThreadId : SV_DispatchThreadID,
    uint3 ml2c_groupId : SV_GroupID,
    uint3 ml2c_groupThreadId : SV_GroupThreadID
)
{
    const uint3 ml2c_numThreads = uint3(32, 1, 1);
    
    const RWBufferStorage storage__encoder3_ResidualBlock_1_residual_func_Split_output_grouped = { ScratchBuffer };
    const Tensor3f8_NHWC< RWBufferStorage > _encoder3_ResidualBlock_1_residual_func_Split_output_grouped = {
        uint3(1920, 1080, 32), // logicalSize
        uint3(0, 0, 0), // threadGroupSliceStart
        uint3(1920, 1080, 32), // threadGroupSliceSize
        uint3(1922, 1082, 32), // storageSize
        uint3(32, 61504, 1), // storageByteStrides
        uint3(1, 1, 0), // paddingBegin
        uint3(1, 1, 0), // paddingEnd
        265804928, // threadGroupStorageByteOffset
        storage__encoder3_ResidualBlock_1_residual_func_Split_output_grouped };
    
    const BufferStorage storage_hwnc__encoder3_ResidualBlock_1_body_spatial_mixing_partial_conv_weight_quant_export_handler_QuantizeLinear_output_0 = { InitializerBuffer };
    const QuantizedTensor4f8_HWNC< BufferStorage > hwnc__encoder3_ResidualBlock_1_body_spatial_mixing_partial_conv_weight_quant_export_handler_QuantizeLinear_output_0 = {
        uint4(3, 3, 16, 16), // logicalSize
        uint4(0, 0, 0, 0), // threadGroupSliceStart
        uint4(3, 3, 16, 16), // threadGroupSliceSize
        uint4(3, 3, 16, 16), // storageSize
        uint4(256, 768, 1, 16), // storageByteStrides
        uint4(0, 0, 0, 0), // paddingBegin
        uint4(0, 0, 0, 0), // paddingEnd
        22312, // threadGroupStorageByteOffset
        1.0, storage_hwnc__encoder3_ResidualBlock_1_body_spatial_mixing_partial_conv_weight_quant_export_handler_QuantizeLinear_output_0 };
    const BufferStorage storage_encoder3_ResidualBlock_1_body_spatial_mixing_partial_conv_bias = { InitializerBuffer };
    const Tensor1f< BufferStorage > encoder3_ResidualBlock_1_body_spatial_mixing_partial_conv_bias = {
        16, // logicalSize
        0, // threadGroupSliceStart
        16, // threadGroupSliceSize
        16, // storageSize
        4, // storageByteStrides
        0, // paddingBegin
        0, // paddingEnd
        2176, // threadGroupStorageByteOffset
        storage_encoder3_ResidualBlock_1_body_spatial_mixing_partial_conv_bias };
    const BufferStorage storage_hwnc__encoder3_ResidualBlock_1_body_pw_expand_weight_quant_export_handler_QuantizeLinear_output_0 = { InitializerBuffer };
    const QuantizedTensor4f8_HWNC< BufferStorage > hwnc__encoder3_ResidualBlock_1_body_pw_expand_weight_quant_export_handler_QuantizeLinear_output_0 = {
        uint4(1, 1, 32, 64), // logicalSize
        uint4(0, 0, 0, 0), // threadGroupSliceStart
        uint4(1, 1, 32, 64), // threadGroupSliceSize
        uint4(1, 1, 32, 64), // storageSize
        uint4(2048, 2048, 1, 32), // storageByteStrides
        uint4(0, 0, 0, 0), // paddingBegin
        uint4(0, 0, 0, 0), // paddingEnd
        24616, // threadGroupStorageByteOffset
        1.0, storage_hwnc__encoder3_ResidualBlock_1_body_pw_expand_weight_quant_export_handler_QuantizeLinear_output_0 };
    const BufferStorage storage_encoder3_ResidualBlock_1_body_pw_expand_bias = { InitializerBuffer };
    const Tensor1f< BufferStorage > encoder3_ResidualBlock_1_body_pw_expand_bias = {
        64, // logicalSize
        0, // threadGroupSliceStart
        64, // threadGroupSliceSize
        64, // storageSize
        4, // storageByteStrides
        0, // paddingBegin
        0, // paddingEnd
        2240, // threadGroupStorageByteOffset
        storage_encoder3_ResidualBlock_1_body_pw_expand_bias };
    const BufferStorage storage_hwnc__encoder3_ResidualBlock_1_body_pw_contract_weight_quant_export_handler_QuantizeLinear_output_0 = { InitializerBuffer };
    const QuantizedTensor4f8_HWNC< BufferStorage > hwnc__encoder3_ResidualBlock_1_body_pw_contract_weight_quant_export_handler_QuantizeLinear_output_0 = {
        uint4(1, 1, 64, 32), // logicalSize
        uint4(0, 0, 0, 0), // threadGroupSliceStart
        uint4(1, 1, 64, 32), // threadGroupSliceSize
        uint4(1, 1, 64, 32), // storageSize
        uint4(2048, 2048, 1, 64), // storageByteStrides
        uint4(0, 0, 0, 0), // paddingBegin
        uint4(0, 0, 0, 0), // paddingEnd
        26664, // threadGroupStorageByteOffset
        1.0, storage_hwnc__encoder3_ResidualBlock_1_body_pw_contract_weight_quant_export_handler_QuantizeLinear_output_0 };
    const BufferStorage storage_encoder3_ResidualBlock_1_body_pw_contract_bias = { InitializerBuffer };
    const Tensor1f< BufferStorage > encoder3_ResidualBlock_1_body_pw_contract_bias = {
        32, // logicalSize
        0, // threadGroupSliceStart
        32, // threadGroupSliceSize
        32, // storageSize
        4, // storageByteStrides
        0, // paddingBegin
        0, // paddingEnd
        2496, // threadGroupStorageByteOffset
        storage_encoder3_ResidualBlock_1_body_pw_contract_bias };
    // fused_quantized_NHWC_/encoder3/DownscaleStridedConv2x2/skip_func/act_quant/export_handler/QuantizeLinear_output_0
    const uint3 logicalSize_slice_10 = uint3(1920, 1080, 32);
    const int3 groupStart_slice_10 = int3(0, 0, 0) + ml2c_groupId.xyz * int3(32, 4, 32);
    const uint3 groupSize_slice_10 = uint3(32, 4, 32);
    const uint3 storageSize_slice_10 = uint3(1922, 1082, 32);
    const uint3 tensorByteStrides_slice_10 = uint3(32, 61504, 1);
    const uint3 paddingBegin_slice_10 = uint3(1, 1, 0);
    const uint3 paddingEnd_slice_10 = uint3(1, 1, 0);
    const int threadGroupByteOffsetInTensor_slice_10 = dot(groupStart_slice_10, tensorByteStrides_slice_10);
    const float quantizationScale_slice_10 = 1.0;
    const RWBufferStorage storage_slice_10 = { ScratchBuffer };
    const QuantizedTensor3f8_NHWC<RWBufferStorage> slice_10 = { logicalSize_slice_10, groupStart_slice_10, groupSize_slice_10, storageSize_slice_10, tensorByteStrides_slice_10, paddingBegin_slice_10, paddingEnd_slice_10, threadGroupByteOffsetInTensor_slice_10 + 132902464, quantizationScale_slice_10, storage_slice_10 };
    // Fusedquantized_/encoder3/ResidualBlock_1/residual_func/Concat_quantized_/encoder3/ResidualBlock_1/body/spatial_mixing/partial_conv/Conv_quantized_/encoder3/ResidualBlock_1/body/spatial_mixing/Concat_quantized_/encoder3/ResidualBlock_1/body/pw_expand/Conv_/encoder3/ResidualBlock_1/body/pw_expand_act/Relu_quantized_/encoder3/ResidualBlock_1/body/pw_contract/Conv_/encoder3/ResidualBlock_1/Add (32, 1080, 1920), (16, 16, 3, 3), (16,), (64, 32, 1, 1), (64,), (32, 64, 1, 1), (32,) -> (32, 1080, 1920)
    ComputeShaderParams computeShaderParams = {ml2c_numThreads, ml2c_groupId, ml2c_groupThreadId, ml2c_dispatchThreadId};
    FasterNetBlock<32, 1>(1.0, 1.0, 1.0, _encoder3_ResidualBlock_1_residual_func_Split_output_grouped, hwnc__encoder3_ResidualBlock_1_body_spatial_mixing_partial_conv_weight_quant_export_handler_QuantizeLinear_output_0, encoder3_ResidualBlock_1_body_spatial_mixing_partial_conv_bias, hwnc__encoder3_ResidualBlock_1_body_pw_expand_weight_quant_export_handler_QuantizeLinear_output_0, encoder3_ResidualBlock_1_body_pw_expand_bias, hwnc__encoder3_ResidualBlock_1_body_pw_contract_weight_quant_export_handler_QuantizeLinear_output_0, encoder3_ResidualBlock_1_body_pw_contract_bias, slice_10, computeShaderParams);
    
}
#endif // #ifdef MLSR_PASS_5
#ifdef MLSR_PASS_5_POST
#define ML2C_GROUPSHARED_SIZE 0

#include "ml2code_runtime/padding.hlsli"
#include "ml2code_runtime/tensor_float8.hlsli"

[numthreads(32, 1, 1)]
void fsr4_model_v07_fp8_no_scale_pass5_post(
    uint3 ml2c_dispatchThreadId : SV_DispatchThreadID,
    uint3 ml2c_groupId : SV_GroupID
)
{
    // fused_quantized_NHWC_/encoder3/DownscaleStridedConv2x2/skip_func/act_quant/export_handler/QuantizeLinear_output_0
    const uint3 logicalSize_slice_11 = uint3(1920, 1080, 32);
    const int3 groupStart_slice_11 = int3(0, 0, 0) + ml2c_groupId.xyz * int3(32, 1, 32);
    const uint3 groupSize_slice_11 = uint3(32, 1, 32);
    const uint3 storageSize_slice_11 = uint3(1922, 1082, 32);
    const uint3 tensorByteStrides_slice_11 = uint3(32, 61504, 1);
    const uint3 paddingBegin_slice_11 = uint3(1, 1, 0);
    const uint3 paddingEnd_slice_11 = uint3(1, 1, 0);
    const int threadGroupByteOffsetInTensor_slice_11 = dot(groupStart_slice_11, tensorByteStrides_slice_11);
    const float quantizationScale_slice_11 = 1.0;
    const RWBufferStorage storage_slice_11 = { ScratchBuffer };
    const QuantizedTensor3f8_NHWC<RWBufferStorage> slice_11 = { logicalSize_slice_11, groupStart_slice_11, groupSize_slice_11, storageSize_slice_11, tensorByteStrides_slice_11, paddingBegin_slice_11, paddingEnd_slice_11, threadGroupByteOffsetInTensor_slice_11 + 132902464, quantizationScale_slice_11, storage_slice_11 };
    
    StoreDefaultConstBatchOperation < 16, QuantizedTensor3f8_NHWC<RWBufferStorage> > batchOp_0 = { DefaultDword };
    ResetPaddingSeparate(slice_11, ml2c_dispatchThreadId, true, true, batchOp_0, (1, 1));
}
#endif // #ifdef MLSR_PASS_5_POST


#ifdef MLSR_PASS_6
#define ML2C_GROUPSHARED_SIZE 0

#include "ml2code_runtime/operators/float8_NHWC/Fused/FusedConv2D_k2s2b_QuantizedOutput.hlsli"

[numthreads(32, 1, 1)]
void fsr4_model_v07_fp8_no_scale_pass6(
    uint3 ml2c_dispatchThreadId : SV_DispatchThreadID,
    uint3 ml2c_groupId : SV_GroupID,
    uint3 ml2c_groupThreadId : SV_GroupThreadID
)
{
    const uint3 ml2c_numThreads = uint3(32, 1, 1);
    
    const RWBufferStorage storage_fused_quantized_NHWC__encoder3_DownscaleStridedConv2x2_skip_func_act_quant_export_handler_QuantizeLinear_output_0 = { ScratchBuffer };
    const QuantizedTensor3f8_NHWC< RWBufferStorage > fused_quantized_NHWC__encoder3_DownscaleStridedConv2x2_skip_func_act_quant_export_handler_QuantizeLinear_output_0 = {
        uint3(1920, 1080, 32), // logicalSize
        uint3(0, 0, 0), // threadGroupSliceStart
        uint3(1920, 1080, 32), // threadGroupSliceSize
        uint3(1922, 1082, 32), // storageSize
        uint3(32, 61504, 1), // storageByteStrides
        uint3(1, 1, 0), // paddingBegin
        uint3(1, 1, 0), // paddingEnd
        132902464, // threadGroupStorageByteOffset
        1.0, storage_fused_quantized_NHWC__encoder3_DownscaleStridedConv2x2_skip_func_act_quant_export_handler_QuantizeLinear_output_0 };
    
    const BufferStorage storage_hwnc__encoder3_DownscaleStridedConv2x2_downscale_conv_weight_quant_export_handler_QuantizeLinear_output_0 = { InitializerBuffer };
    const QuantizedTensor4f8_HWNC< BufferStorage > hwnc__encoder3_DownscaleStridedConv2x2_downscale_conv_weight_quant_export_handler_QuantizeLinear_output_0 = {
        uint4(2, 2, 32, 64), // logicalSize
        uint4(0, 0, 0, 0), // threadGroupSliceStart
        uint4(2, 2, 32, 64), // threadGroupSliceSize
        uint4(2, 2, 32, 64), // storageSize
        uint4(2048, 4096, 1, 32), // storageByteStrides
        uint4(0, 0, 0, 0), // paddingBegin
        uint4(0, 0, 0, 0), // paddingEnd
        28712, // threadGroupStorageByteOffset
        1.0, storage_hwnc__encoder3_DownscaleStridedConv2x2_downscale_conv_weight_quant_export_handler_QuantizeLinear_output_0 };
    const BufferStorage storage_encoder3_DownscaleStridedConv2x2_downscale_conv_bias = { InitializerBuffer };
    const Tensor1f< BufferStorage > encoder3_DownscaleStridedConv2x2_downscale_conv_bias = {
        64, // logicalSize
        0, // threadGroupSliceStart
        64, // threadGroupSliceSize
        64, // storageSize
        4, // storageByteStrides
        0, // paddingBegin
        0, // paddingEnd
        2624, // threadGroupStorageByteOffset
        storage_encoder3_DownscaleStridedConv2x2_downscale_conv_bias };
    // Fusedquantized_/encoder3/DownscaleStridedConv2x2/downscale_conv/Conv_quantized_outputs_output_grouped
    const uint3 logicalSize_slice_12 = uint3(960, 540, 64);
    const int3 groupStart_slice_12 = int3(0, 0, 0) + ml2c_groupId.xyz * int3(32, 1, 64);
    const uint3 groupSize_slice_12 = uint3(32, 1, 64);
    const uint3 storageSize_slice_12 = uint3(962, 542, 64);
    const uint3 tensorByteStrides_slice_12 = uint3(64, 61568, 1);
    const uint3 paddingBegin_slice_12 = uint3(1, 1, 0);
    const uint3 paddingEnd_slice_12 = uint3(1, 1, 0);
    const int threadGroupByteOffsetInTensor_slice_12 = dot(groupStart_slice_12, tensorByteStrides_slice_12);
    const RWBufferStorage storage_slice_12 = { ScratchBuffer };
    const Tensor3f8_NHWC<RWBufferStorage> slice_12 = { logicalSize_slice_12, groupStart_slice_12, groupSize_slice_12, storageSize_slice_12, tensorByteStrides_slice_12, paddingBegin_slice_12, paddingEnd_slice_12, threadGroupByteOffsetInTensor_slice_12 + 199449792, storage_slice_12 };
    // Fusedquantized_/encoder3/DownscaleStridedConv2x2/downscale_conv/Conv_quantized_outputs (32, 1080, 1920), (64, 32, 2, 2), (64,) -> (64, 540, 960)
    ComputeShaderParams computeShaderParams = {ml2c_numThreads, ml2c_groupId, ml2c_groupThreadId, ml2c_dispatchThreadId};
    FusedConv2D_k2s2b_QuantizedOutput(1.0, 1.0, fused_quantized_NHWC__encoder3_DownscaleStridedConv2x2_skip_func_act_quant_export_handler_QuantizeLinear_output_0, hwnc__encoder3_DownscaleStridedConv2x2_downscale_conv_weight_quant_export_handler_QuantizeLinear_output_0, encoder3_DownscaleStridedConv2x2_downscale_conv_bias, slice_12, computeShaderParams);
    
}
#endif // #ifdef MLSR_PASS_6
#ifdef MLSR_PASS_6_POST
#define ML2C_GROUPSHARED_SIZE 0

#include "ml2code_runtime/padding.hlsli"
#include "ml2code_runtime/tensor_float8.hlsli"

[numthreads(32, 1, 1)]
void fsr4_model_v07_fp8_no_scale_pass6_post(
    uint3 ml2c_dispatchThreadId : SV_DispatchThreadID,
    uint3 ml2c_groupId : SV_GroupID
)
{
    // Fusedquantized_/encoder3/DownscaleStridedConv2x2/downscale_conv/Conv_quantized_outputs_output_grouped
    const uint3 logicalSize_slice_13 = uint3(960, 540, 64);
    const int3 groupStart_slice_13 = int3(0, 0, 0) + ml2c_groupId.xyz * int3(32, 1, 64);
    const uint3 groupSize_slice_13 = uint3(32, 1, 64);
    const uint3 storageSize_slice_13 = uint3(962, 542, 64);
    const uint3 tensorByteStrides_slice_13 = uint3(64, 61568, 1);
    const uint3 paddingBegin_slice_13 = uint3(1, 1, 0);
    const uint3 paddingEnd_slice_13 = uint3(1, 1, 0);
    const int threadGroupByteOffsetInTensor_slice_13 = dot(groupStart_slice_13, tensorByteStrides_slice_13);
    const RWBufferStorage storage_slice_13 = { ScratchBuffer };
    const Tensor3f8_NHWC<RWBufferStorage> slice_13 = { logicalSize_slice_13, groupStart_slice_13, groupSize_slice_13, storageSize_slice_13, tensorByteStrides_slice_13, paddingBegin_slice_13, paddingEnd_slice_13, threadGroupByteOffsetInTensor_slice_13 + 199449792, storage_slice_13 };
    
    StoreDefaultConstBatchOperation < 16, Tensor3f8_NHWC<RWBufferStorage> > batchOp_0 = { DefaultDword };
    ResetPaddingSeparate(slice_13, ml2c_dispatchThreadId, true, true, batchOp_0, (1, 1));
}
#endif // #ifdef MLSR_PASS_6_POST


#ifdef MLSR_PASS_7
#define ML2C_GROUPSHARED_SIZE 0

#include "ml2code_runtime/operators/float8_NHWC/Fused/FasterNetBlock.hlsli"

[numthreads(32, 1, 1)]
void fsr4_model_v07_fp8_no_scale_pass7(
    uint3 ml2c_dispatchThreadId : SV_DispatchThreadID,
    uint3 ml2c_groupId : SV_GroupID,
    uint3 ml2c_groupThreadId : SV_GroupThreadID
)
{
    const uint3 ml2c_numThreads = uint3(32, 1, 1);
    
    const RWBufferStorage storage_Fusedquantized__encoder3_DownscaleStridedConv2x2_downscale_conv_Conv_quantized_outputs_output_grouped = { ScratchBuffer };
    const Tensor3f8_NHWC< RWBufferStorage > Fusedquantized__encoder3_DownscaleStridedConv2x2_downscale_conv_Conv_quantized_outputs_output_grouped = {
        uint3(960, 540, 64), // logicalSize
        uint3(0, 0, 0), // threadGroupSliceStart
        uint3(960, 540, 64), // threadGroupSliceSize
        uint3(962, 542, 64), // storageSize
        uint3(64, 61568, 1), // storageByteStrides
        uint3(1, 1, 0), // paddingBegin
        uint3(1, 1, 0), // paddingEnd
        199449792, // threadGroupStorageByteOffset
        storage_Fusedquantized__encoder3_DownscaleStridedConv2x2_downscale_conv_Conv_quantized_outputs_output_grouped };
    
    const BufferStorage storage_hwnc__bottleneck_ResidualBlock_0_body_spatial_mixing_partial_conv_weight_quant_export_handler_QuantizeLinear_output_0 = { InitializerBuffer };
    const QuantizedTensor4f8_HWNC< BufferStorage > hwnc__bottleneck_ResidualBlock_0_body_spatial_mixing_partial_conv_weight_quant_export_handler_QuantizeLinear_output_0 = {
        uint4(3, 3, 16, 32), // logicalSize
        uint4(0, 0, 0, 0), // threadGroupSliceStart
        uint4(3, 3, 16, 32), // threadGroupSliceSize
        uint4(3, 3, 16, 32), // storageSize
        uint4(512, 1536, 1, 16), // storageByteStrides
        uint4(0, 0, 0, 0), // paddingBegin
        uint4(0, 0, 0, 0), // paddingEnd
        36904, // threadGroupStorageByteOffset
        1.0, storage_hwnc__bottleneck_ResidualBlock_0_body_spatial_mixing_partial_conv_weight_quant_export_handler_QuantizeLinear_output_0 };
    const BufferStorage storage_bottleneck_ResidualBlock_0_body_spatial_mixing_partial_conv_bias = { InitializerBuffer };
    const Tensor1f< BufferStorage > bottleneck_ResidualBlock_0_body_spatial_mixing_partial_conv_bias = {
        32, // logicalSize
        0, // threadGroupSliceStart
        32, // threadGroupSliceSize
        32, // storageSize
        4, // storageByteStrides
        0, // paddingBegin
        0, // paddingEnd
        2880, // threadGroupStorageByteOffset
        storage_bottleneck_ResidualBlock_0_body_spatial_mixing_partial_conv_bias };
    const BufferStorage storage_hwnc__bottleneck_ResidualBlock_0_body_pw_expand_weight_quant_export_handler_QuantizeLinear_output_0 = { InitializerBuffer };
    const QuantizedTensor4f8_HWNC< BufferStorage > hwnc__bottleneck_ResidualBlock_0_body_pw_expand_weight_quant_export_handler_QuantizeLinear_output_0 = {
        uint4(1, 1, 64, 128), // logicalSize
        uint4(0, 0, 0, 0), // threadGroupSliceStart
        uint4(1, 1, 64, 128), // threadGroupSliceSize
        uint4(1, 1, 64, 128), // storageSize
        uint4(8192, 8192, 1, 64), // storageByteStrides
        uint4(0, 0, 0, 0), // paddingBegin
        uint4(0, 0, 0, 0), // paddingEnd
        41512, // threadGroupStorageByteOffset
        1.0, storage_hwnc__bottleneck_ResidualBlock_0_body_pw_expand_weight_quant_export_handler_QuantizeLinear_output_0 };
    const BufferStorage storage_bottleneck_ResidualBlock_0_body_pw_expand_bias = { InitializerBuffer };
    const Tensor1f< BufferStorage > bottleneck_ResidualBlock_0_body_pw_expand_bias = {
        128, // logicalSize
        0, // threadGroupSliceStart
        128, // threadGroupSliceSize
        128, // storageSize
        4, // storageByteStrides
        0, // paddingBegin
        0, // paddingEnd
        3008, // threadGroupStorageByteOffset
        storage_bottleneck_ResidualBlock_0_body_pw_expand_bias };
    const BufferStorage storage_hwnc__bottleneck_ResidualBlock_0_body_pw_contract_weight_quant_export_handler_QuantizeLinear_output_0 = { InitializerBuffer };
    const QuantizedTensor4f8_HWNC< BufferStorage > hwnc__bottleneck_ResidualBlock_0_body_pw_contract_weight_quant_export_handler_QuantizeLinear_output_0 = {
        uint4(1, 1, 128, 64), // logicalSize
        uint4(0, 0, 0, 0), // threadGroupSliceStart
        uint4(1, 1, 128, 64), // threadGroupSliceSize
        uint4(1, 1, 128, 64), // storageSize
        uint4(8192, 8192, 1, 128), // storageByteStrides
        uint4(0, 0, 0, 0), // paddingBegin
        uint4(0, 0, 0, 0), // paddingEnd
        49704, // threadGroupStorageByteOffset
        1.0, storage_hwnc__bottleneck_ResidualBlock_0_body_pw_contract_weight_quant_export_handler_QuantizeLinear_output_0 };
    const BufferStorage storage_bottleneck_ResidualBlock_0_body_pw_contract_bias = { InitializerBuffer };
    const Tensor1f< BufferStorage > bottleneck_ResidualBlock_0_body_pw_contract_bias = {
        64, // logicalSize
        0, // threadGroupSliceStart
        64, // threadGroupSliceSize
        64, // storageSize
        4, // storageByteStrides
        0, // paddingBegin
        0, // paddingEnd
        3520, // threadGroupStorageByteOffset
        storage_bottleneck_ResidualBlock_0_body_pw_contract_bias };
    // /bottleneck/ResidualBlock_1/residual_func/Split_output_grouped
    const uint3 logicalSize_slice_14 = uint3(960, 540, 64);
    const int3 groupStart_slice_14 = int3(0, 0, 0) + ml2c_groupId.xyz * int3(32, 1, 64);
    const uint3 groupSize_slice_14 = uint3(32, 1, 64);
    const uint3 storageSize_slice_14 = uint3(962, 542, 64);
    const uint3 tensorByteStrides_slice_14 = uint3(64, 61568, 1);
    const uint3 paddingBegin_slice_14 = uint3(1, 1, 0);
    const uint3 paddingEnd_slice_14 = uint3(1, 1, 0);
    const int threadGroupByteOffsetInTensor_slice_14 = dot(groupStart_slice_14, tensorByteStrides_slice_14);
    const RWBufferStorage storage_slice_14 = { ScratchBuffer };
    const Tensor3f8_NHWC<RWBufferStorage> slice_14 = { logicalSize_slice_14, groupStart_slice_14, groupSize_slice_14, storageSize_slice_14, tensorByteStrides_slice_14, paddingBegin_slice_14, paddingEnd_slice_14, threadGroupByteOffsetInTensor_slice_14 + 232819648, storage_slice_14 };
    // Fusedquantized_/bottleneck/ResidualBlock_0/residual_func/Concat_quantized_/bottleneck/ResidualBlock_0/body/spatial_mixing/partial_conv/Conv_quantized_/bottleneck/ResidualBlock_0/body/spatial_mixing/Concat_quantized_/bottleneck/ResidualBlock_0/body/pw_expand/Conv_/bottleneck/ResidualBlock_0/body/pw_expand_act/Relu_quantized_/bottleneck/ResidualBlock_0/body/pw_contract/Conv_/bottleneck/ResidualBlock_0/Add (64, 540, 960), (32, 16, 3, 3), (32,), (128, 64, 1, 1), (128,), (64, 128, 1, 1), (64,) -> (64, 540, 960)
    ComputeShaderParams computeShaderParams = {ml2c_numThreads, ml2c_groupId, ml2c_groupThreadId, ml2c_dispatchThreadId};
    FasterNetBlock<64, 2>(1.0, 1.0, 1.0, 1.0, 1.0, Fusedquantized__encoder3_DownscaleStridedConv2x2_downscale_conv_Conv_quantized_outputs_output_grouped, hwnc__bottleneck_ResidualBlock_0_body_spatial_mixing_partial_conv_weight_quant_export_handler_QuantizeLinear_output_0, bottleneck_ResidualBlock_0_body_spatial_mixing_partial_conv_bias, hwnc__bottleneck_ResidualBlock_0_body_pw_expand_weight_quant_export_handler_QuantizeLinear_output_0, bottleneck_ResidualBlock_0_body_pw_expand_bias, hwnc__bottleneck_ResidualBlock_0_body_pw_contract_weight_quant_export_handler_QuantizeLinear_output_0, bottleneck_ResidualBlock_0_body_pw_contract_bias, slice_14, computeShaderParams);
    
}
#endif // #ifdef MLSR_PASS_7
#ifdef MLSR_PASS_7_POST
#define ML2C_GROUPSHARED_SIZE 0

#include "ml2code_runtime/padding.hlsli"
#include "ml2code_runtime/tensor_float8.hlsli"

[numthreads(32, 1, 1)]
void fsr4_model_v07_fp8_no_scale_pass7_post(
    uint3 ml2c_dispatchThreadId : SV_DispatchThreadID,
    uint3 ml2c_groupId : SV_GroupID
)
{
    // /bottleneck/ResidualBlock_1/residual_func/Split_output_grouped
    const uint3 logicalSize_slice_15 = uint3(960, 540, 64);
    const int3 groupStart_slice_15 = int3(0, 0, 0) + ml2c_groupId.xyz * int3(32, 1, 64);
    const uint3 groupSize_slice_15 = uint3(32, 1, 64);
    const uint3 storageSize_slice_15 = uint3(962, 542, 64);
    const uint3 tensorByteStrides_slice_15 = uint3(64, 61568, 1);
    const uint3 paddingBegin_slice_15 = uint3(1, 1, 0);
    const uint3 paddingEnd_slice_15 = uint3(1, 1, 0);
    const int threadGroupByteOffsetInTensor_slice_15 = dot(groupStart_slice_15, tensorByteStrides_slice_15);
    const RWBufferStorage storage_slice_15 = { ScratchBuffer };
    const Tensor3f8_NHWC<RWBufferStorage> slice_15 = { logicalSize_slice_15, groupStart_slice_15, groupSize_slice_15, storageSize_slice_15, tensorByteStrides_slice_15, paddingBegin_slice_15, paddingEnd_slice_15, threadGroupByteOffsetInTensor_slice_15 + 232819648, storage_slice_15 };
    
    StoreDefaultConstBatchOperation < 16, Tensor3f8_NHWC<RWBufferStorage> > batchOp_0 = { DefaultDword };
    ResetPaddingSeparate(slice_15, ml2c_dispatchThreadId, true, true, batchOp_0, (1, 1));
}
#endif // #ifdef MLSR_PASS_7_POST


#ifdef MLSR_PASS_8
#define ML2C_GROUPSHARED_SIZE 0

#include "ml2code_runtime/operators/float8_NHWC/Fused/FasterNetBlock.hlsli"

[numthreads(32, 1, 1)]
void fsr4_model_v07_fp8_no_scale_pass8(
    uint3 ml2c_dispatchThreadId : SV_DispatchThreadID,
    uint3 ml2c_groupId : SV_GroupID,
    uint3 ml2c_groupThreadId : SV_GroupThreadID
)
{
    const uint3 ml2c_numThreads = uint3(32, 1, 1);
    
    const RWBufferStorage storage__bottleneck_ResidualBlock_1_residual_func_Split_output_grouped = { ScratchBuffer };
    const Tensor3f8_NHWC< RWBufferStorage > _bottleneck_ResidualBlock_1_residual_func_Split_output_grouped = {
        uint3(960, 540, 64), // logicalSize
        uint3(0, 0, 0), // threadGroupSliceStart
        uint3(960, 540, 64), // threadGroupSliceSize
        uint3(962, 542, 64), // storageSize
        uint3(64, 61568, 1), // storageByteStrides
        uint3(1, 1, 0), // paddingBegin
        uint3(1, 1, 0), // paddingEnd
        232819648, // threadGroupStorageByteOffset
        storage__bottleneck_ResidualBlock_1_residual_func_Split_output_grouped };
    
    const BufferStorage storage_hwnc__bottleneck_ResidualBlock_1_body_spatial_mixing_partial_conv_weight_quant_export_handler_QuantizeLinear_output_0 = { InitializerBuffer };
    const QuantizedTensor4f8_HWNC< BufferStorage > hwnc__bottleneck_ResidualBlock_1_body_spatial_mixing_partial_conv_weight_quant_export_handler_QuantizeLinear_output_0 = {
        uint4(3, 3, 16, 32), // logicalSize
        uint4(0, 0, 0, 0), // threadGroupSliceStart
        uint4(3, 3, 16, 32), // threadGroupSliceSize
        uint4(3, 3, 16, 32), // storageSize
        uint4(512, 1536, 1, 16), // storageByteStrides
        uint4(0, 0, 0, 0), // paddingBegin
        uint4(0, 0, 0, 0), // paddingEnd
        57896, // threadGroupStorageByteOffset
        1.0, storage_hwnc__bottleneck_ResidualBlock_1_body_spatial_mixing_partial_conv_weight_quant_export_handler_QuantizeLinear_output_0 };
    const BufferStorage storage_bottleneck_ResidualBlock_1_body_spatial_mixing_partial_conv_bias = { InitializerBuffer };
    const Tensor1f< BufferStorage > bottleneck_ResidualBlock_1_body_spatial_mixing_partial_conv_bias = {
        32, // logicalSize
        0, // threadGroupSliceStart
        32, // threadGroupSliceSize
        32, // storageSize
        4, // storageByteStrides
        0, // paddingBegin
        0, // paddingEnd
        3776, // threadGroupStorageByteOffset
        storage_bottleneck_ResidualBlock_1_body_spatial_mixing_partial_conv_bias };
    const BufferStorage storage_hwnc__bottleneck_ResidualBlock_1_body_pw_expand_weight_quant_export_handler_QuantizeLinear_output_0 = { InitializerBuffer };
    const QuantizedTensor4f8_HWNC< BufferStorage > hwnc__bottleneck_ResidualBlock_1_body_pw_expand_weight_quant_export_handler_QuantizeLinear_output_0 = {
        uint4(1, 1, 64, 128), // logicalSize
        uint4(0, 0, 0, 0), // threadGroupSliceStart
        uint4(1, 1, 64, 128), // threadGroupSliceSize
        uint4(1, 1, 64, 128), // storageSize
        uint4(8192, 8192, 1, 64), // storageByteStrides
        uint4(0, 0, 0, 0), // paddingBegin
        uint4(0, 0, 0, 0), // paddingEnd
        62504, // threadGroupStorageByteOffset
        1.0, storage_hwnc__bottleneck_ResidualBlock_1_body_pw_expand_weight_quant_export_handler_QuantizeLinear_output_0 };
    const BufferStorage storage_bottleneck_ResidualBlock_1_body_pw_expand_bias = { InitializerBuffer };
    const Tensor1f< BufferStorage > bottleneck_ResidualBlock_1_body_pw_expand_bias = {
        128, // logicalSize
        0, // threadGroupSliceStart
        128, // threadGroupSliceSize
        128, // storageSize
        4, // storageByteStrides
        0, // paddingBegin
        0, // paddingEnd
        3904, // threadGroupStorageByteOffset
        storage_bottleneck_ResidualBlock_1_body_pw_expand_bias };
    const BufferStorage storage_hwnc__bottleneck_ResidualBlock_1_body_pw_contract_weight_quant_export_handler_QuantizeLinear_output_0 = { InitializerBuffer };
    const QuantizedTensor4f8_HWNC< BufferStorage > hwnc__bottleneck_ResidualBlock_1_body_pw_contract_weight_quant_export_handler_QuantizeLinear_output_0 = {
        uint4(1, 1, 128, 64), // logicalSize
        uint4(0, 0, 0, 0), // threadGroupSliceStart
        uint4(1, 1, 128, 64), // threadGroupSliceSize
        uint4(1, 1, 128, 64), // storageSize
        uint4(8192, 8192, 1, 128), // storageByteStrides
        uint4(0, 0, 0, 0), // paddingBegin
        uint4(0, 0, 0, 0), // paddingEnd
        70696, // threadGroupStorageByteOffset
        1.0, storage_hwnc__bottleneck_ResidualBlock_1_body_pw_contract_weight_quant_export_handler_QuantizeLinear_output_0 };
    const BufferStorage storage_bottleneck_ResidualBlock_1_body_pw_contract_bias = { InitializerBuffer };
    const Tensor1f< BufferStorage > bottleneck_ResidualBlock_1_body_pw_contract_bias = {
        64, // logicalSize
        0, // threadGroupSliceStart
        64, // threadGroupSliceSize
        64, // storageSize
        4, // storageByteStrides
        0, // paddingBegin
        0, // paddingEnd
        4416, // threadGroupStorageByteOffset
        storage_bottleneck_ResidualBlock_1_body_pw_contract_bias };
    // /bottleneck/ResidualBlock_2/residual_func/Split_output_grouped
    const uint3 logicalSize_slice_16 = uint3(960, 540, 64);
    const int3 groupStart_slice_16 = int3(0, 0, 0) + ml2c_groupId.xyz * int3(32, 1, 64);
    const uint3 groupSize_slice_16 = uint3(32, 1, 64);
    const uint3 storageSize_slice_16 = uint3(962, 542, 64);
    const uint3 tensorByteStrides_slice_16 = uint3(64, 61568, 1);
    const uint3 paddingBegin_slice_16 = uint3(1, 1, 0);
    const uint3 paddingEnd_slice_16 = uint3(1, 1, 0);
    const int threadGroupByteOffsetInTensor_slice_16 = dot(groupStart_slice_16, tensorByteStrides_slice_16);
    const RWBufferStorage storage_slice_16 = { ScratchBuffer };
    const Tensor3f8_NHWC<RWBufferStorage> slice_16 = { logicalSize_slice_16, groupStart_slice_16, groupSize_slice_16, storageSize_slice_16, tensorByteStrides_slice_16, paddingBegin_slice_16, paddingEnd_slice_16, threadGroupByteOffsetInTensor_slice_16 + 199449792, storage_slice_16 };
    // Fusedquantized_/bottleneck/ResidualBlock_1/residual_func/Concat_quantized_/bottleneck/ResidualBlock_1/body/spatial_mixing/partial_conv/Conv_quantized_/bottleneck/ResidualBlock_1/body/spatial_mixing/Concat_quantized_/bottleneck/ResidualBlock_1/body/pw_expand/Conv_/bottleneck/ResidualBlock_1/body/pw_expand_act/Relu_quantized_/bottleneck/ResidualBlock_1/body/pw_contract/Conv_/bottleneck/ResidualBlock_1/Add (64, 540, 960), (32, 16, 3, 3), (32,), (128, 64, 1, 1), (128,), (64, 128, 1, 1), (64,) -> (64, 540, 960)
    ComputeShaderParams computeShaderParams = {ml2c_numThreads, ml2c_groupId, ml2c_groupThreadId, ml2c_dispatchThreadId};
    FasterNetBlock<64, 2>(1.0, 1.0, 1.0, 1.0, 1.0, _bottleneck_ResidualBlock_1_residual_func_Split_output_grouped, hwnc__bottleneck_ResidualBlock_1_body_spatial_mixing_partial_conv_weight_quant_export_handler_QuantizeLinear_output_0, bottleneck_ResidualBlock_1_body_spatial_mixing_partial_conv_bias, hwnc__bottleneck_ResidualBlock_1_body_pw_expand_weight_quant_export_handler_QuantizeLinear_output_0, bottleneck_ResidualBlock_1_body_pw_expand_bias, hwnc__bottleneck_ResidualBlock_1_body_pw_contract_weight_quant_export_handler_QuantizeLinear_output_0, bottleneck_ResidualBlock_1_body_pw_contract_bias, slice_16, computeShaderParams);
    
}
#endif // #ifdef MLSR_PASS_8
#ifdef MLSR_PASS_8_POST
#define ML2C_GROUPSHARED_SIZE 0

#include "ml2code_runtime/padding.hlsli"
#include "ml2code_runtime/tensor_float8.hlsli"

[numthreads(32, 1, 1)]
void fsr4_model_v07_fp8_no_scale_pass8_post(
    uint3 ml2c_dispatchThreadId : SV_DispatchThreadID,
    uint3 ml2c_groupId : SV_GroupID
)
{
    // /bottleneck/ResidualBlock_2/residual_func/Split_output_grouped
    const uint3 logicalSize_slice_17 = uint3(960, 540, 64);
    const int3 groupStart_slice_17 = int3(0, 0, 0) + ml2c_groupId.xyz * int3(32, 1, 64);
    const uint3 groupSize_slice_17 = uint3(32, 1, 64);
    const uint3 storageSize_slice_17 = uint3(962, 542, 64);
    const uint3 tensorByteStrides_slice_17 = uint3(64, 61568, 1);
    const uint3 paddingBegin_slice_17 = uint3(1, 1, 0);
    const uint3 paddingEnd_slice_17 = uint3(1, 1, 0);
    const int threadGroupByteOffsetInTensor_slice_17 = dot(groupStart_slice_17, tensorByteStrides_slice_17);
    const RWBufferStorage storage_slice_17 = { ScratchBuffer };
    const Tensor3f8_NHWC<RWBufferStorage> slice_17 = { logicalSize_slice_17, groupStart_slice_17, groupSize_slice_17, storageSize_slice_17, tensorByteStrides_slice_17, paddingBegin_slice_17, paddingEnd_slice_17, threadGroupByteOffsetInTensor_slice_17 + 199449792, storage_slice_17 };
    
    StoreDefaultConstBatchOperation < 16, Tensor3f8_NHWC<RWBufferStorage> > batchOp_0 = { DefaultDword };
    ResetPaddingSeparate(slice_17, ml2c_dispatchThreadId, true, true, batchOp_0, (1, 1));
}
#endif // #ifdef MLSR_PASS_8_POST


#ifdef MLSR_PASS_9
#define ML2C_GROUPSHARED_SIZE 0

#include "ml2code_runtime/operators/float8_NHWC/Fused/FNB_CT2D_ADD.hlsli"

[numthreads(32, 1, 1)]
void fsr4_model_v07_fp8_no_scale_pass9(
    uint3 ml2c_dispatchThreadId : SV_DispatchThreadID,
    uint3 ml2c_groupId : SV_GroupID,
    uint3 ml2c_groupThreadId : SV_GroupThreadID
)
{
    const uint3 ml2c_numThreads = uint3(32, 1, 1);
    
    const RWBufferStorage storage__bottleneck_ResidualBlock_2_residual_func_Split_output_grouped = { ScratchBuffer };
    const Tensor3f8_NHWC< RWBufferStorage > _bottleneck_ResidualBlock_2_residual_func_Split_output_grouped = {
        uint3(960, 540, 64), // logicalSize
        uint3(0, 0, 0), // threadGroupSliceStart
        uint3(960, 540, 64), // threadGroupSliceSize
        uint3(962, 542, 64), // storageSize
        uint3(64, 61568, 1), // storageByteStrides
        uint3(1, 1, 0), // paddingBegin
        uint3(1, 1, 0), // paddingEnd
        199449792, // threadGroupStorageByteOffset
        storage__bottleneck_ResidualBlock_2_residual_func_Split_output_grouped };
    
    const RWBufferStorage storage_fused_quantized_NHWC__encoder3_DownscaleStridedConv2x2_skip_func_act_quant_export_handler_QuantizeLinear_output_0 = { ScratchBuffer };
    const QuantizedTensor3f8_NHWC< RWBufferStorage > fused_quantized_NHWC__encoder3_DownscaleStridedConv2x2_skip_func_act_quant_export_handler_QuantizeLinear_output_0 = {
        uint3(1920, 1080, 32), // logicalSize
        uint3(0, 0, 0), // threadGroupSliceStart
        uint3(1920, 1080, 32), // threadGroupSliceSize
        uint3(1922, 1082, 32), // storageSize
        uint3(32, 61504, 1), // storageByteStrides
        uint3(1, 1, 0), // paddingBegin
        uint3(1, 1, 0), // paddingEnd
        132902464, // threadGroupStorageByteOffset
        1.0, storage_fused_quantized_NHWC__encoder3_DownscaleStridedConv2x2_skip_func_act_quant_export_handler_QuantizeLinear_output_0 };
    
    const BufferStorage storage_hwnc__bottleneck_ResidualBlock_2_body_spatial_mixing_partial_conv_weight_quant_export_handler_QuantizeLinear_output_0 = { InitializerBuffer };
    const QuantizedTensor4f8_HWNC< BufferStorage > hwnc__bottleneck_ResidualBlock_2_body_spatial_mixing_partial_conv_weight_quant_export_handler_QuantizeLinear_output_0 = {
        uint4(3, 3, 16, 32), // logicalSize
        uint4(0, 0, 0, 0), // threadGroupSliceStart
        uint4(3, 3, 16, 32), // threadGroupSliceSize
        uint4(3, 3, 16, 32), // storageSize
        uint4(512, 1536, 1, 16), // storageByteStrides
        uint4(0, 0, 0, 0), // paddingBegin
        uint4(0, 0, 0, 0), // paddingEnd
        78888, // threadGroupStorageByteOffset
        1.0, storage_hwnc__bottleneck_ResidualBlock_2_body_spatial_mixing_partial_conv_weight_quant_export_handler_QuantizeLinear_output_0 };
    const BufferStorage storage_bottleneck_ResidualBlock_2_body_spatial_mixing_partial_conv_bias = { InitializerBuffer };
    const Tensor1f< BufferStorage > bottleneck_ResidualBlock_2_body_spatial_mixing_partial_conv_bias = {
        32, // logicalSize
        0, // threadGroupSliceStart
        32, // threadGroupSliceSize
        32, // storageSize
        4, // storageByteStrides
        0, // paddingBegin
        0, // paddingEnd
        4672, // threadGroupStorageByteOffset
        storage_bottleneck_ResidualBlock_2_body_spatial_mixing_partial_conv_bias };
    const BufferStorage storage_hwnc__bottleneck_ResidualBlock_2_body_pw_expand_weight_quant_export_handler_QuantizeLinear_output_0 = { InitializerBuffer };
    const QuantizedTensor4f8_HWNC< BufferStorage > hwnc__bottleneck_ResidualBlock_2_body_pw_expand_weight_quant_export_handler_QuantizeLinear_output_0 = {
        uint4(1, 1, 64, 128), // logicalSize
        uint4(0, 0, 0, 0), // threadGroupSliceStart
        uint4(1, 1, 64, 128), // threadGroupSliceSize
        uint4(1, 1, 64, 128), // storageSize
        uint4(8192, 8192, 1, 64), // storageByteStrides
        uint4(0, 0, 0, 0), // paddingBegin
        uint4(0, 0, 0, 0), // paddingEnd
        83496, // threadGroupStorageByteOffset
        1.0, storage_hwnc__bottleneck_ResidualBlock_2_body_pw_expand_weight_quant_export_handler_QuantizeLinear_output_0 };
    const BufferStorage storage_bottleneck_ResidualBlock_2_body_pw_expand_bias = { InitializerBuffer };
    const Tensor1f< BufferStorage > bottleneck_ResidualBlock_2_body_pw_expand_bias = {
        128, // logicalSize
        0, // threadGroupSliceStart
        128, // threadGroupSliceSize
        128, // storageSize
        4, // storageByteStrides
        0, // paddingBegin
        0, // paddingEnd
        4800, // threadGroupStorageByteOffset
        storage_bottleneck_ResidualBlock_2_body_pw_expand_bias };
    const BufferStorage storage_hwnc__bottleneck_ResidualBlock_2_body_pw_contract_weight_quant_export_handler_QuantizeLinear_output_0 = { InitializerBuffer };
    const QuantizedTensor4f8_HWNC< BufferStorage > hwnc__bottleneck_ResidualBlock_2_body_pw_contract_weight_quant_export_handler_QuantizeLinear_output_0 = {
        uint4(1, 1, 128, 64), // logicalSize
        uint4(0, 0, 0, 0), // threadGroupSliceStart
        uint4(1, 1, 128, 64), // threadGroupSliceSize
        uint4(1, 1, 128, 64), // storageSize
        uint4(8192, 8192, 1, 128), // storageByteStrides
        uint4(0, 0, 0, 0), // paddingBegin
        uint4(0, 0, 0, 0), // paddingEnd
        91688, // threadGroupStorageByteOffset
        1.0, storage_hwnc__bottleneck_ResidualBlock_2_body_pw_contract_weight_quant_export_handler_QuantizeLinear_output_0 };
    const BufferStorage storage_bottleneck_ResidualBlock_2_body_pw_contract_bias = { InitializerBuffer };
    const Tensor1f< BufferStorage > bottleneck_ResidualBlock_2_body_pw_contract_bias = {
        64, // logicalSize
        0, // threadGroupSliceStart
        64, // threadGroupSliceSize
        64, // storageSize
        4, // storageByteStrides
        0, // paddingBegin
        0, // paddingEnd
        5312, // threadGroupStorageByteOffset
        storage_bottleneck_ResidualBlock_2_body_pw_contract_bias };
    const BufferStorage storage_hwcn__bottleneck_UpscaleConvTranspose2x2_upscale_conv_weight_quant_export_handler_QuantizeLinear_output_0 = { InitializerBuffer };
    const QuantizedTensor4f8_HWCN< BufferStorage > hwcn__bottleneck_UpscaleConvTranspose2x2_upscale_conv_weight_quant_export_handler_QuantizeLinear_output_0 = {
        uint4(2, 2, 32, 64), // logicalSize
        uint4(0, 0, 0, 0), // threadGroupSliceStart
        uint4(2, 2, 32, 64), // threadGroupSliceSize
        uint4(2, 2, 32, 64), // storageSize
        uint4(2048, 4096, 64, 1), // storageByteStrides
        uint4(0, 0, 0, 0), // paddingBegin
        uint4(0, 0, 0, 0), // paddingEnd
        119336, // threadGroupStorageByteOffset
        1.0, storage_hwcn__bottleneck_UpscaleConvTranspose2x2_upscale_conv_weight_quant_export_handler_QuantizeLinear_output_0 };
    const BufferStorage storage_bottleneck_UpscaleConvTranspose2x2_upscale_conv_bias = { InitializerBuffer };
    const Tensor1f< BufferStorage > bottleneck_UpscaleConvTranspose2x2_upscale_conv_bias = {
        32, // logicalSize
        0, // threadGroupSliceStart
        32, // threadGroupSliceSize
        32, // storageSize
        4, // storageByteStrides
        0, // paddingBegin
        0, // paddingEnd
        5568, // threadGroupStorageByteOffset
        storage_bottleneck_UpscaleConvTranspose2x2_upscale_conv_bias };
    // FusedFusedquantized_/bottleneck/ResidualBlock_2/residual_func/Concat_quantized_/bottleneck/ResidualBlock_2/body/spatial_mixing/partial_conv/Conv_quantized_/bottleneck/ResidualBlock_2/body/spatial_mixing/Concat_quantized_/bottleneck/ResidualBlock_2/body/pw_expand/Conv_/bottleneck/ResidualBlock_2/body/pw_expand_act/Relu_quantized_/bottleneck/ResidualBlock_2/body/pw_contract/Conv_/bottleneck/ResidualBlock_2/Add_Fusedquantized_/bottleneck/UpscaleConvTranspose2x2/upscale_conv/ConvTranspose_Fusedquantized_/decoder3/skip_pop_0/Add_QuantizedOutput_output_grouped
    const uint3 logicalSize_slice_18 = uint3(1920, 1080, 32);
    const int3 groupStart_slice_18 = int3(0, 0, 0) + ml2c_groupId.xyz * int3(64, 2, 32);
    const uint3 groupSize_slice_18 = uint3(64, 2, 32);
    const uint3 storageSize_slice_18 = uint3(1922, 1082, 32);
    const uint3 tensorByteStrides_slice_18 = uint3(32, 61504, 1);
    const uint3 paddingBegin_slice_18 = uint3(1, 1, 0);
    const uint3 paddingEnd_slice_18 = uint3(1, 1, 0);
    const int threadGroupByteOffsetInTensor_slice_18 = dot(groupStart_slice_18, tensorByteStrides_slice_18);
    const RWBufferStorage storage_slice_18 = { ScratchBuffer };
    const Tensor3f8_NHWC<RWBufferStorage> slice_18 = { logicalSize_slice_18, groupStart_slice_18, groupSize_slice_18, storageSize_slice_18, tensorByteStrides_slice_18, paddingBegin_slice_18, paddingEnd_slice_18, threadGroupByteOffsetInTensor_slice_18 + 232819648, storage_slice_18 };
    // FusedFusedquantized_/bottleneck/ResidualBlock_2/residual_func/Concat_quantized_/bottleneck/ResidualBlock_2/body/spatial_mixing/partial_conv/Conv_quantized_/bottleneck/ResidualBlock_2/body/spatial_mixing/Concat_quantized_/bottleneck/ResidualBlock_2/body/pw_expand/Conv_/bottleneck/ResidualBlock_2/body/pw_expand_act/Relu_quantized_/bottleneck/ResidualBlock_2/body/pw_contract/Conv_/bottleneck/ResidualBlock_2/Add_Fusedquantized_/bottleneck/UpscaleConvTranspose2x2/upscale_conv/ConvTranspose_Fusedquantized_/decoder3/skip_pop_0/Add_QuantizedOutput (64, 540, 960), (32, 1080, 1920), (32, 16, 3, 3), (32,), (128, 64, 1, 1), (128,), (64, 128, 1, 1), (64,), (64, 32, 2, 2), (32,) -> (32, 1080, 1920)
    ComputeShaderParams computeShaderParams = {ml2c_numThreads, ml2c_groupId, ml2c_groupThreadId, ml2c_dispatchThreadId};
    FNB_CT2D_ADD<64, 2>(1.0, 1.0, 1.0, 1.0, 1.0, 1.0, _bottleneck_ResidualBlock_2_residual_func_Split_output_grouped, fused_quantized_NHWC__encoder3_DownscaleStridedConv2x2_skip_func_act_quant_export_handler_QuantizeLinear_output_0, hwnc__bottleneck_ResidualBlock_2_body_spatial_mixing_partial_conv_weight_quant_export_handler_QuantizeLinear_output_0, bottleneck_ResidualBlock_2_body_spatial_mixing_partial_conv_bias, hwnc__bottleneck_ResidualBlock_2_body_pw_expand_weight_quant_export_handler_QuantizeLinear_output_0, bottleneck_ResidualBlock_2_body_pw_expand_bias, hwnc__bottleneck_ResidualBlock_2_body_pw_contract_weight_quant_export_handler_QuantizeLinear_output_0, bottleneck_ResidualBlock_2_body_pw_contract_bias, hwcn__bottleneck_UpscaleConvTranspose2x2_upscale_conv_weight_quant_export_handler_QuantizeLinear_output_0, bottleneck_UpscaleConvTranspose2x2_upscale_conv_bias, slice_18, computeShaderParams);
    
}
#endif // #ifdef MLSR_PASS_9
#ifdef MLSR_PASS_9_POST
#define ML2C_GROUPSHARED_SIZE 0

#include "ml2code_runtime/padding.hlsli"
#include "ml2code_runtime/tensor_float8.hlsli"

[numthreads(32, 1, 1)]
void fsr4_model_v07_fp8_no_scale_pass9_post(
    uint3 ml2c_dispatchThreadId : SV_DispatchThreadID,
    uint3 ml2c_groupId : SV_GroupID
)
{
    // FusedFusedquantized_/bottleneck/ResidualBlock_2/residual_func/Concat_quantized_/bottleneck/ResidualBlock_2/body/spatial_mixing/partial_conv/Conv_quantized_/bottleneck/ResidualBlock_2/body/spatial_mixing/Concat_quantized_/bottleneck/ResidualBlock_2/body/pw_expand/Conv_/bottleneck/ResidualBlock_2/body/pw_expand_act/Relu_quantized_/bottleneck/ResidualBlock_2/body/pw_contract/Conv_/bottleneck/ResidualBlock_2/Add_Fusedquantized_/bottleneck/UpscaleConvTranspose2x2/upscale_conv/ConvTranspose_Fusedquantized_/decoder3/skip_pop_0/Add_QuantizedOutput_output_grouped
    const uint3 logicalSize_slice_19 = uint3(1920, 1080, 32);
    const int3 groupStart_slice_19 = int3(0, 0, 0) + ml2c_groupId.xyz * int3(32, 1, 32);
    const uint3 groupSize_slice_19 = uint3(32, 1, 32);
    const uint3 storageSize_slice_19 = uint3(1922, 1082, 32);
    const uint3 tensorByteStrides_slice_19 = uint3(32, 61504, 1);
    const uint3 paddingBegin_slice_19 = uint3(1, 1, 0);
    const uint3 paddingEnd_slice_19 = uint3(1, 1, 0);
    const int threadGroupByteOffsetInTensor_slice_19 = dot(groupStart_slice_19, tensorByteStrides_slice_19);
    const RWBufferStorage storage_slice_19 = { ScratchBuffer };
    const Tensor3f8_NHWC<RWBufferStorage> slice_19 = { logicalSize_slice_19, groupStart_slice_19, groupSize_slice_19, storageSize_slice_19, tensorByteStrides_slice_19, paddingBegin_slice_19, paddingEnd_slice_19, threadGroupByteOffsetInTensor_slice_19 + 232819648, storage_slice_19 };
    
    StoreDefaultConstBatchOperation < 16, Tensor3f8_NHWC<RWBufferStorage> > batchOp_0 = { DefaultDword };
    ResetPaddingSeparate(slice_19, ml2c_dispatchThreadId, true, true, batchOp_0, (1, 1));
}
#endif // #ifdef MLSR_PASS_9_POST


#ifdef MLSR_PASS_10
#define ML2C_GROUPSHARED_SIZE 0

#include "ml2code_runtime/operators/float8_NHWC/Fused/FasterNetBlock.hlsli"

[numthreads(32, 1, 1)]
void fsr4_model_v07_fp8_no_scale_pass10(
    uint3 ml2c_dispatchThreadId : SV_DispatchThreadID,
    uint3 ml2c_groupId : SV_GroupID,
    uint3 ml2c_groupThreadId : SV_GroupThreadID
)
{
    const uint3 ml2c_numThreads = uint3(32, 1, 1);
    
    const RWBufferStorage storage_FusedFusedquantized__bottleneck_ResidualBlock_2_residual_func_Concat_quantized__bottleneck_ResidualBlock_2_body_spatial_mixing_partial_conv_Conv_quantized__bottleneck_ResidualBlock_2_body_spatial_mixing_Concat_quantized__bottleneck_ResidualBlock_2_body_pw_expand_Conv__bottleneck_ResidualBlock_2_body_pw_expand_act_Relu_quantized__bottleneck_ResidualBlock_2_body_pw_contract_Conv__bottleneck_ResidualBlock_2_Add_Fusedquantized__bottleneck_UpscaleConvTranspose2x2_upscale_conv_ConvTranspose_Fusedquantized__decoder3_skip_pop_0_Add_QuantizedOutput_output_grouped = { ScratchBuffer };
    const Tensor3f8_NHWC< RWBufferStorage > FusedFusedquantized__bottleneck_ResidualBlock_2_residual_func_Concat_quantized__bottleneck_ResidualBlock_2_body_spatial_mixing_partial_conv_Conv_quantized__bottleneck_ResidualBlock_2_body_spatial_mixing_Concat_quantized__bottleneck_ResidualBlock_2_body_pw_expand_Conv__bottleneck_ResidualBlock_2_body_pw_expand_act_Relu_quantized__bottleneck_ResidualBlock_2_body_pw_contract_Conv__bottleneck_ResidualBlock_2_Add_Fusedquantized__bottleneck_UpscaleConvTranspose2x2_upscale_conv_ConvTranspose_Fusedquantized__decoder3_skip_pop_0_Add_QuantizedOutput_output_grouped = {
        uint3(1920, 1080, 32), // logicalSize
        uint3(0, 0, 0), // threadGroupSliceStart
        uint3(1920, 1080, 32), // threadGroupSliceSize
        uint3(1922, 1082, 32), // storageSize
        uint3(32, 61504, 1), // storageByteStrides
        uint3(1, 1, 0), // paddingBegin
        uint3(1, 1, 0), // paddingEnd
        232819648, // threadGroupStorageByteOffset
        storage_FusedFusedquantized__bottleneck_ResidualBlock_2_residual_func_Concat_quantized__bottleneck_ResidualBlock_2_body_spatial_mixing_partial_conv_Conv_quantized__bottleneck_ResidualBlock_2_body_spatial_mixing_Concat_quantized__bottleneck_ResidualBlock_2_body_pw_expand_Conv__bottleneck_ResidualBlock_2_body_pw_expand_act_Relu_quantized__bottleneck_ResidualBlock_2_body_pw_contract_Conv__bottleneck_ResidualBlock_2_Add_Fusedquantized__bottleneck_UpscaleConvTranspose2x2_upscale_conv_ConvTranspose_Fusedquantized__decoder3_skip_pop_0_Add_QuantizedOutput_output_grouped };
    
    const BufferStorage storage_hwnc__decoder3_ResidualBlock_1_body_spatial_mixing_partial_conv_weight_quant_export_handler_QuantizeLinear_output_0 = { InitializerBuffer };
    const QuantizedTensor4f8_HWNC< BufferStorage > hwnc__decoder3_ResidualBlock_1_body_spatial_mixing_partial_conv_weight_quant_export_handler_QuantizeLinear_output_0 = {
        uint4(3, 3, 16, 16), // logicalSize
        uint4(0, 0, 0, 0), // threadGroupSliceStart
        uint4(3, 3, 16, 16), // threadGroupSliceSize
        uint4(3, 3, 16, 16), // storageSize
        uint4(256, 768, 1, 16), // storageByteStrides
        uint4(0, 0, 0, 0), // paddingBegin
        uint4(0, 0, 0, 0), // paddingEnd
        99880, // threadGroupStorageByteOffset
        1.0, storage_hwnc__decoder3_ResidualBlock_1_body_spatial_mixing_partial_conv_weight_quant_export_handler_QuantizeLinear_output_0 };
    const BufferStorage storage_decoder3_ResidualBlock_1_body_spatial_mixing_partial_conv_bias = { InitializerBuffer };
    const Tensor1f< BufferStorage > decoder3_ResidualBlock_1_body_spatial_mixing_partial_conv_bias = {
        16, // logicalSize
        0, // threadGroupSliceStart
        16, // threadGroupSliceSize
        16, // storageSize
        4, // storageByteStrides
        0, // paddingBegin
        0, // paddingEnd
        5696, // threadGroupStorageByteOffset
        storage_decoder3_ResidualBlock_1_body_spatial_mixing_partial_conv_bias };
    const BufferStorage storage_hwnc__decoder3_ResidualBlock_1_body_pw_expand_weight_quant_export_handler_QuantizeLinear_output_0 = { InitializerBuffer };
    const QuantizedTensor4f8_HWNC< BufferStorage > hwnc__decoder3_ResidualBlock_1_body_pw_expand_weight_quant_export_handler_QuantizeLinear_output_0 = {
        uint4(1, 1, 32, 64), // logicalSize
        uint4(0, 0, 0, 0), // threadGroupSliceStart
        uint4(1, 1, 32, 64), // threadGroupSliceSize
        uint4(1, 1, 32, 64), // storageSize
        uint4(2048, 2048, 1, 32), // storageByteStrides
        uint4(0, 0, 0, 0), // paddingBegin
        uint4(0, 0, 0, 0), // paddingEnd
        102184, // threadGroupStorageByteOffset
        1.0, storage_hwnc__decoder3_ResidualBlock_1_body_pw_expand_weight_quant_export_handler_QuantizeLinear_output_0 };
    const BufferStorage storage_decoder3_ResidualBlock_1_body_pw_expand_bias = { InitializerBuffer };
    const Tensor1f< BufferStorage > decoder3_ResidualBlock_1_body_pw_expand_bias = {
        64, // logicalSize
        0, // threadGroupSliceStart
        64, // threadGroupSliceSize
        64, // storageSize
        4, // storageByteStrides
        0, // paddingBegin
        0, // paddingEnd
        5760, // threadGroupStorageByteOffset
        storage_decoder3_ResidualBlock_1_body_pw_expand_bias };
    const BufferStorage storage_hwnc__decoder3_ResidualBlock_1_body_pw_contract_weight_quant_export_handler_QuantizeLinear_output_0 = { InitializerBuffer };
    const QuantizedTensor4f8_HWNC< BufferStorage > hwnc__decoder3_ResidualBlock_1_body_pw_contract_weight_quant_export_handler_QuantizeLinear_output_0 = {
        uint4(1, 1, 64, 32), // logicalSize
        uint4(0, 0, 0, 0), // threadGroupSliceStart
        uint4(1, 1, 64, 32), // threadGroupSliceSize
        uint4(1, 1, 64, 32), // storageSize
        uint4(2048, 2048, 1, 64), // storageByteStrides
        uint4(0, 0, 0, 0), // paddingBegin
        uint4(0, 0, 0, 0), // paddingEnd
        104232, // threadGroupStorageByteOffset
        1.0, storage_hwnc__decoder3_ResidualBlock_1_body_pw_contract_weight_quant_export_handler_QuantizeLinear_output_0 };
    const BufferStorage storage_decoder3_ResidualBlock_1_body_pw_contract_bias = { InitializerBuffer };
    const Tensor1f< BufferStorage > decoder3_ResidualBlock_1_body_pw_contract_bias = {
        32, // logicalSize
        0, // threadGroupSliceStart
        32, // threadGroupSliceSize
        32, // storageSize
        4, // storageByteStrides
        0, // paddingBegin
        0, // paddingEnd
        6016, // threadGroupStorageByteOffset
        storage_decoder3_ResidualBlock_1_body_pw_contract_bias };
    // /decoder3/ResidualBlock_2/residual_func/Split_output_grouped
    const uint3 logicalSize_slice_20 = uint3(1920, 1080, 32);
    const int3 groupStart_slice_20 = int3(0, 0, 0) + ml2c_groupId.xyz * int3(32, 4, 32);
    const uint3 groupSize_slice_20 = uint3(32, 4, 32);
    const uint3 storageSize_slice_20 = uint3(1922, 1082, 32);
    const uint3 tensorByteStrides_slice_20 = uint3(32, 61504, 1);
    const uint3 paddingBegin_slice_20 = uint3(1, 1, 0);
    const uint3 paddingEnd_slice_20 = uint3(1, 1, 0);
    const int threadGroupByteOffsetInTensor_slice_20 = dot(groupStart_slice_20, tensorByteStrides_slice_20);
    const RWBufferStorage storage_slice_20 = { ScratchBuffer };
    const Tensor3f8_NHWC<RWBufferStorage> slice_20 = { logicalSize_slice_20, groupStart_slice_20, groupSize_slice_20, storageSize_slice_20, tensorByteStrides_slice_20, paddingBegin_slice_20, paddingEnd_slice_20, threadGroupByteOffsetInTensor_slice_20 + 132902464, storage_slice_20 };
    // Fusedquantized_/decoder3/ResidualBlock_1/residual_func/Concat_quantized_/decoder3/ResidualBlock_1/body/spatial_mixing/partial_conv/Conv_quantized_/decoder3/ResidualBlock_1/body/spatial_mixing/Concat_quantized_/decoder3/ResidualBlock_1/body/pw_expand/Conv_/decoder3/ResidualBlock_1/body/pw_expand_act/Relu_quantized_/decoder3/ResidualBlock_1/body/pw_contract/Conv_/decoder3/ResidualBlock_1/Add (32, 1080, 1920), (16, 16, 3, 3), (16,), (64, 32, 1, 1), (64,), (32, 64, 1, 1), (32,) -> (32, 1080, 1920)
    ComputeShaderParams computeShaderParams = {ml2c_numThreads, ml2c_groupId, ml2c_groupThreadId, ml2c_dispatchThreadId};
    FasterNetBlock<32, 1>(1.0, 1.0, 1.0, 1.0, 1.0, FusedFusedquantized__bottleneck_ResidualBlock_2_residual_func_Concat_quantized__bottleneck_ResidualBlock_2_body_spatial_mixing_partial_conv_Conv_quantized__bottleneck_ResidualBlock_2_body_spatial_mixing_Concat_quantized__bottleneck_ResidualBlock_2_body_pw_expand_Conv__bottleneck_ResidualBlock_2_body_pw_expand_act_Relu_quantized__bottleneck_ResidualBlock_2_body_pw_contract_Conv__bottleneck_ResidualBlock_2_Add_Fusedquantized__bottleneck_UpscaleConvTranspose2x2_upscale_conv_ConvTranspose_Fusedquantized__decoder3_skip_pop_0_Add_QuantizedOutput_output_grouped, hwnc__decoder3_ResidualBlock_1_body_spatial_mixing_partial_conv_weight_quant_export_handler_QuantizeLinear_output_0, decoder3_ResidualBlock_1_body_spatial_mixing_partial_conv_bias, hwnc__decoder3_ResidualBlock_1_body_pw_expand_weight_quant_export_handler_QuantizeLinear_output_0, decoder3_ResidualBlock_1_body_pw_expand_bias, hwnc__decoder3_ResidualBlock_1_body_pw_contract_weight_quant_export_handler_QuantizeLinear_output_0, decoder3_ResidualBlock_1_body_pw_contract_bias, slice_20, computeShaderParams);
    
}
#endif // #ifdef MLSR_PASS_10
#ifdef MLSR_PASS_10_POST
#define ML2C_GROUPSHARED_SIZE 0

#include "ml2code_runtime/padding.hlsli"
#include "ml2code_runtime/tensor_float8.hlsli"

[numthreads(32, 1, 1)]
void fsr4_model_v07_fp8_no_scale_pass10_post(
    uint3 ml2c_dispatchThreadId : SV_DispatchThreadID,
    uint3 ml2c_groupId : SV_GroupID
)
{
    // /decoder3/ResidualBlock_2/residual_func/Split_output_grouped
    const uint3 logicalSize_slice_21 = uint3(1920, 1080, 32);
    const int3 groupStart_slice_21 = int3(0, 0, 0) + ml2c_groupId.xyz * int3(32, 1, 32);
    const uint3 groupSize_slice_21 = uint3(32, 1, 32);
    const uint3 storageSize_slice_21 = uint3(1922, 1082, 32);
    const uint3 tensorByteStrides_slice_21 = uint3(32, 61504, 1);
    const uint3 paddingBegin_slice_21 = uint3(1, 1, 0);
    const uint3 paddingEnd_slice_21 = uint3(1, 1, 0);
    const int threadGroupByteOffsetInTensor_slice_21 = dot(groupStart_slice_21, tensorByteStrides_slice_21);
    const RWBufferStorage storage_slice_21 = { ScratchBuffer };
    const Tensor3f8_NHWC<RWBufferStorage> slice_21 = { logicalSize_slice_21, groupStart_slice_21, groupSize_slice_21, storageSize_slice_21, tensorByteStrides_slice_21, paddingBegin_slice_21, paddingEnd_slice_21, threadGroupByteOffsetInTensor_slice_21 + 132902464, storage_slice_21 };
    
    StoreDefaultConstBatchOperation < 16, Tensor3f8_NHWC<RWBufferStorage> > batchOp_0 = { DefaultDword };
    ResetPaddingSeparate(slice_21, ml2c_dispatchThreadId, true, true, batchOp_0, (1, 1));
}
#endif // #ifdef MLSR_PASS_10_POST


#ifdef MLSR_PASS_11
#define ML2C_GROUPSHARED_SIZE 0

#include "ml2code_runtime/operators/float8_NHWC/Fused/FNB_CT2D_ADD.hlsli"

[numthreads(32, 1, 1)]
void fsr4_model_v07_fp8_no_scale_pass11(
    uint3 ml2c_dispatchThreadId : SV_DispatchThreadID,
    uint3 ml2c_groupId : SV_GroupID,
    uint3 ml2c_groupThreadId : SV_GroupThreadID
)
{
    const uint3 ml2c_numThreads = uint3(32, 1, 1);
    
    const RWBufferStorage storage__decoder3_ResidualBlock_2_residual_func_Split_output_grouped = { ScratchBuffer };
    const Tensor3f8_NHWC< RWBufferStorage > _decoder3_ResidualBlock_2_residual_func_Split_output_grouped = {
        uint3(1920, 1080, 32), // logicalSize
        uint3(0, 0, 0), // threadGroupSliceStart
        uint3(1920, 1080, 32), // threadGroupSliceSize
        uint3(1922, 1082, 32), // storageSize
        uint3(32, 61504, 1), // storageByteStrides
        uint3(1, 1, 0), // paddingBegin
        uint3(1, 1, 0), // paddingEnd
        132902464, // threadGroupStorageByteOffset
        storage__decoder3_ResidualBlock_2_residual_func_Split_output_grouped };
    
    const RWBufferStorage storage_fused_quantized_NHWC__encoder2_DownscaleStridedConv2x2_skip_func_act_quant_export_handler_QuantizeLinear_output_0 = { ScratchBuffer };
    const QuantizedTensor3f8_NHWC< RWBufferStorage > fused_quantized_NHWC__encoder2_DownscaleStridedConv2x2_skip_func_act_quant_export_handler_QuantizeLinear_output_0 = {
        uint3(3840, 2160, 16), // logicalSize
        uint3(0, 0, 0), // threadGroupSliceStart
        uint3(3840, 2160, 16), // threadGroupSliceSize
        uint3(3842, 2162, 16), // storageSize
        uint3(16, 61472, 1), // storageByteStrides
        uint3(1, 1, 0), // paddingBegin
        uint3(1, 1, 0), // paddingEnd
        0, // threadGroupStorageByteOffset
        1.0, storage_fused_quantized_NHWC__encoder2_DownscaleStridedConv2x2_skip_func_act_quant_export_handler_QuantizeLinear_output_0 };
    
    const BufferStorage storage_hwnc__decoder3_ResidualBlock_2_body_spatial_mixing_partial_conv_weight_quant_export_handler_QuantizeLinear_output_0 = { InitializerBuffer };
    const QuantizedTensor4f8_HWNC< BufferStorage > hwnc__decoder3_ResidualBlock_2_body_spatial_mixing_partial_conv_weight_quant_export_handler_QuantizeLinear_output_0 = {
        uint4(3, 3, 16, 16), // logicalSize
        uint4(0, 0, 0, 0), // threadGroupSliceStart
        uint4(3, 3, 16, 16), // threadGroupSliceSize
        uint4(3, 3, 16, 16), // storageSize
        uint4(256, 768, 1, 16), // storageByteStrides
        uint4(0, 0, 0, 0), // paddingBegin
        uint4(0, 0, 0, 0), // paddingEnd
        106280, // threadGroupStorageByteOffset
        1.0, storage_hwnc__decoder3_ResidualBlock_2_body_spatial_mixing_partial_conv_weight_quant_export_handler_QuantizeLinear_output_0 };
    const BufferStorage storage_decoder3_ResidualBlock_2_body_spatial_mixing_partial_conv_bias = { InitializerBuffer };
    const Tensor1f< BufferStorage > decoder3_ResidualBlock_2_body_spatial_mixing_partial_conv_bias = {
        16, // logicalSize
        0, // threadGroupSliceStart
        16, // threadGroupSliceSize
        16, // storageSize
        4, // storageByteStrides
        0, // paddingBegin
        0, // paddingEnd
        6144, // threadGroupStorageByteOffset
        storage_decoder3_ResidualBlock_2_body_spatial_mixing_partial_conv_bias };
    const BufferStorage storage_hwnc__decoder3_ResidualBlock_2_body_pw_expand_weight_quant_export_handler_QuantizeLinear_output_0 = { InitializerBuffer };
    const QuantizedTensor4f8_HWNC< BufferStorage > hwnc__decoder3_ResidualBlock_2_body_pw_expand_weight_quant_export_handler_QuantizeLinear_output_0 = {
        uint4(1, 1, 32, 64), // logicalSize
        uint4(0, 0, 0, 0), // threadGroupSliceStart
        uint4(1, 1, 32, 64), // threadGroupSliceSize
        uint4(1, 1, 32, 64), // storageSize
        uint4(2048, 2048, 1, 32), // storageByteStrides
        uint4(0, 0, 0, 0), // paddingBegin
        uint4(0, 0, 0, 0), // paddingEnd
        108584, // threadGroupStorageByteOffset
        1.0, storage_hwnc__decoder3_ResidualBlock_2_body_pw_expand_weight_quant_export_handler_QuantizeLinear_output_0 };
    const BufferStorage storage_decoder3_ResidualBlock_2_body_pw_expand_bias = { InitializerBuffer };
    const Tensor1f< BufferStorage > decoder3_ResidualBlock_2_body_pw_expand_bias = {
        64, // logicalSize
        0, // threadGroupSliceStart
        64, // threadGroupSliceSize
        64, // storageSize
        4, // storageByteStrides
        0, // paddingBegin
        0, // paddingEnd
        6208, // threadGroupStorageByteOffset
        storage_decoder3_ResidualBlock_2_body_pw_expand_bias };
    const BufferStorage storage_hwnc__decoder3_ResidualBlock_2_body_pw_contract_weight_quant_export_handler_QuantizeLinear_output_0 = { InitializerBuffer };
    const QuantizedTensor4f8_HWNC< BufferStorage > hwnc__decoder3_ResidualBlock_2_body_pw_contract_weight_quant_export_handler_QuantizeLinear_output_0 = {
        uint4(1, 1, 64, 32), // logicalSize
        uint4(0, 0, 0, 0), // threadGroupSliceStart
        uint4(1, 1, 64, 32), // threadGroupSliceSize
        uint4(1, 1, 64, 32), // storageSize
        uint4(2048, 2048, 1, 64), // storageByteStrides
        uint4(0, 0, 0, 0), // paddingBegin
        uint4(0, 0, 0, 0), // paddingEnd
        110632, // threadGroupStorageByteOffset
        1.0, storage_hwnc__decoder3_ResidualBlock_2_body_pw_contract_weight_quant_export_handler_QuantizeLinear_output_0 };
    const BufferStorage storage_decoder3_ResidualBlock_2_body_pw_contract_bias = { InitializerBuffer };
    const Tensor1f< BufferStorage > decoder3_ResidualBlock_2_body_pw_contract_bias = {
        32, // logicalSize
        0, // threadGroupSliceStart
        32, // threadGroupSliceSize
        32, // storageSize
        4, // storageByteStrides
        0, // paddingBegin
        0, // paddingEnd
        6464, // threadGroupStorageByteOffset
        storage_decoder3_ResidualBlock_2_body_pw_contract_bias };
    const BufferStorage storage_hwcn__decoder3_UpscaleConvTranspose2x2_upscale_conv_weight_quant_export_handler_QuantizeLinear_output_0 = { InitializerBuffer };
    const QuantizedTensor4f8_HWCN< BufferStorage > hwcn__decoder3_UpscaleConvTranspose2x2_upscale_conv_weight_quant_export_handler_QuantizeLinear_output_0 = {
        uint4(2, 2, 16, 32), // logicalSize
        uint4(0, 0, 0, 0), // threadGroupSliceStart
        uint4(2, 2, 16, 32), // threadGroupSliceSize
        uint4(2, 2, 16, 32), // storageSize
        uint4(512, 1024, 32, 1), // storageByteStrides
        uint4(0, 0, 0, 0), // paddingBegin
        uint4(0, 0, 0, 0), // paddingEnd
        127528, // threadGroupStorageByteOffset
        1.0, storage_hwcn__decoder3_UpscaleConvTranspose2x2_upscale_conv_weight_quant_export_handler_QuantizeLinear_output_0 };
    const BufferStorage storage_decoder3_UpscaleConvTranspose2x2_upscale_conv_bias = { InitializerBuffer };
    const Tensor1f< BufferStorage > decoder3_UpscaleConvTranspose2x2_upscale_conv_bias = {
        16, // logicalSize
        0, // threadGroupSliceStart
        16, // threadGroupSliceSize
        16, // storageSize
        4, // storageByteStrides
        0, // paddingBegin
        0, // paddingEnd
        6592, // threadGroupStorageByteOffset
        storage_decoder3_UpscaleConvTranspose2x2_upscale_conv_bias };
    // fused_fused_quantized_NHWC_/decoder2/ResidualBlock_1/body/input_quantization/act_quant/export_handler/QuantizeLinear_output_0
    const uint3 logicalSize_slice_22 = uint3(3840, 2160, 16);
    const int3 groupStart_slice_22 = int3(0, 0, 0) + ml2c_groupId.xyz * int3(64, 2, 16);
    const uint3 groupSize_slice_22 = uint3(64, 2, 16);
    const uint3 storageSize_slice_22 = uint3(3842, 2162, 16);
    const uint3 tensorByteStrides_slice_22 = uint3(16, 61472, 1);
    const uint3 paddingBegin_slice_22 = uint3(1, 1, 0);
    const uint3 paddingEnd_slice_22 = uint3(1, 1, 0);
    const int threadGroupByteOffsetInTensor_slice_22 = dot(groupStart_slice_22, tensorByteStrides_slice_22);
    const float quantizationScale_slice_22 = 1.0;
    const RWBufferStorage storage_slice_22 = { ScratchBuffer };
    const QuantizedTensor3f8_NHWC<RWBufferStorage> slice_22 = { logicalSize_slice_22, groupStart_slice_22, groupSize_slice_22, storageSize_slice_22, tensorByteStrides_slice_22, paddingBegin_slice_22, paddingEnd_slice_22, threadGroupByteOffsetInTensor_slice_22 + 199449792, quantizationScale_slice_22, storage_slice_22 };
    // FusedFusedquantized_/decoder3/ResidualBlock_2/residual_func/Concat_quantized_/decoder3/ResidualBlock_2/body/spatial_mixing/partial_conv/Conv_quantized_/decoder3/ResidualBlock_2/body/spatial_mixing/Concat_quantized_/decoder3/ResidualBlock_2/body/pw_expand/Conv_/decoder3/ResidualBlock_2/body/pw_expand_act/Relu_quantized_/decoder3/ResidualBlock_2/body/pw_contract/Conv_/decoder3/ResidualBlock_2/Add_Fusedquantized_/decoder3/UpscaleConvTranspose2x2/upscale_conv/ConvTranspose_quantized_/decoder2/skip_pop_0/Add (32, 1080, 1920), (16, 2160, 3840), (16, 16, 3, 3), (16,), (64, 32, 1, 1), (64,), (32, 64, 1, 1), (32,), (32, 16, 2, 2), (16,) -> (16, 2160, 3840)
    ComputeShaderParams computeShaderParams = {ml2c_numThreads, ml2c_groupId, ml2c_groupThreadId, ml2c_dispatchThreadId};
    FNB_CT2D_ADD<32, 1>(1.0, 1.0, 1.0, 1.0, _decoder3_ResidualBlock_2_residual_func_Split_output_grouped, fused_quantized_NHWC__encoder2_DownscaleStridedConv2x2_skip_func_act_quant_export_handler_QuantizeLinear_output_0, hwnc__decoder3_ResidualBlock_2_body_spatial_mixing_partial_conv_weight_quant_export_handler_QuantizeLinear_output_0, decoder3_ResidualBlock_2_body_spatial_mixing_partial_conv_bias, hwnc__decoder3_ResidualBlock_2_body_pw_expand_weight_quant_export_handler_QuantizeLinear_output_0, decoder3_ResidualBlock_2_body_pw_expand_bias, hwnc__decoder3_ResidualBlock_2_body_pw_contract_weight_quant_export_handler_QuantizeLinear_output_0, decoder3_ResidualBlock_2_body_pw_contract_bias, hwcn__decoder3_UpscaleConvTranspose2x2_upscale_conv_weight_quant_export_handler_QuantizeLinear_output_0, decoder3_UpscaleConvTranspose2x2_upscale_conv_bias, slice_22, computeShaderParams);
    
}
#endif // #ifdef MLSR_PASS_11
#ifdef MLSR_PASS_11_POST
#define ML2C_GROUPSHARED_SIZE 0

#include "ml2code_runtime/padding.hlsli"
#include "ml2code_runtime/tensor_float8.hlsli"

[numthreads(32, 1, 1)]
void fsr4_model_v07_fp8_no_scale_pass11_post(
    uint3 ml2c_dispatchThreadId : SV_DispatchThreadID,
    uint3 ml2c_groupId : SV_GroupID
)
{
    // fused_fused_quantized_NHWC_/decoder2/ResidualBlock_1/body/input_quantization/act_quant/export_handler/QuantizeLinear_output_0
    const uint3 logicalSize_slice_23 = uint3(3840, 2160, 16);
    const int3 groupStart_slice_23 = int3(0, 0, 0) + ml2c_groupId.xyz * int3(32, 1, 16);
    const uint3 groupSize_slice_23 = uint3(32, 1, 16);
    const uint3 storageSize_slice_23 = uint3(3842, 2162, 16);
    const uint3 tensorByteStrides_slice_23 = uint3(16, 61472, 1);
    const uint3 paddingBegin_slice_23 = uint3(1, 1, 0);
    const uint3 paddingEnd_slice_23 = uint3(1, 1, 0);
    const int threadGroupByteOffsetInTensor_slice_23 = dot(groupStart_slice_23, tensorByteStrides_slice_23);
    const float quantizationScale_slice_23 = 1.0;
    const RWBufferStorage storage_slice_23 = { ScratchBuffer };
    const QuantizedTensor3f8_NHWC<RWBufferStorage> slice_23 = { logicalSize_slice_23, groupStart_slice_23, groupSize_slice_23, storageSize_slice_23, tensorByteStrides_slice_23, paddingBegin_slice_23, paddingEnd_slice_23, threadGroupByteOffsetInTensor_slice_23 + 199449792, quantizationScale_slice_23, storage_slice_23 };
    
    StoreDefaultConstBatchOperation < 16, QuantizedTensor3f8_NHWC<RWBufferStorage> > batchOp_0 = { DefaultDword };
    ResetPaddingSeparate(slice_23, ml2c_dispatchThreadId, true, true, batchOp_0, (1, 1));
}
#endif // #ifdef MLSR_PASS_11_POST


#ifdef MLSR_PASS_12
#define ML2C_GROUPSHARED_SIZE 0

#include "ml2code_runtime/operators/float8_NHWC/Fused/ConvNextBlock.hlsli"

[numthreads(32, 1, 1)]
void fsr4_model_v07_fp8_no_scale_pass12(
    uint3 ml2c_dispatchThreadId : SV_DispatchThreadID,
    uint3 ml2c_groupId : SV_GroupID,
    uint3 ml2c_groupThreadId : SV_GroupThreadID
)
{
    const uint3 ml2c_numThreads = uint3(32, 1, 1);
    
    const RWBufferStorage storage_fused_fused_quantized_NHWC__decoder2_ResidualBlock_1_body_input_quantization_act_quant_export_handler_QuantizeLinear_output_0 = { ScratchBuffer };
    const QuantizedTensor3f8_NHWC< RWBufferStorage > fused_fused_quantized_NHWC__decoder2_ResidualBlock_1_body_input_quantization_act_quant_export_handler_QuantizeLinear_output_0 = {
        uint3(3840, 2160, 16), // logicalSize
        uint3(0, 0, 0), // threadGroupSliceStart
        uint3(3840, 2160, 16), // threadGroupSliceSize
        uint3(3842, 2162, 16), // storageSize
        uint3(16, 61472, 1), // storageByteStrides
        uint3(1, 1, 0), // paddingBegin
        uint3(1, 1, 0), // paddingEnd
        199449792, // threadGroupStorageByteOffset
        1.0, storage_fused_fused_quantized_NHWC__decoder2_ResidualBlock_1_body_input_quantization_act_quant_export_handler_QuantizeLinear_output_0 };
    
    const BufferStorage storage_hwnc__decoder2_ResidualBlock_1_body_conv_dw_weight_quant_export_handler_QuantizeLinear_output_0 = { InitializerBuffer };
    const QuantizedTensor4f8_HWNC< BufferStorage > hwnc__decoder2_ResidualBlock_1_body_conv_dw_weight_quant_export_handler_QuantizeLinear_output_0 = {
        uint4(3, 3, 16, 16), // logicalSize
        uint4(0, 0, 0, 0), // threadGroupSliceStart
        uint4(3, 3, 16, 16), // threadGroupSliceSize
        uint4(3, 3, 16, 16), // storageSize
        uint4(256, 768, 1, 16), // storageByteStrides
        uint4(0, 0, 0, 0), // paddingBegin
        uint4(0, 0, 0, 0), // paddingEnd
        112680, // threadGroupStorageByteOffset
        1.0, storage_hwnc__decoder2_ResidualBlock_1_body_conv_dw_weight_quant_export_handler_QuantizeLinear_output_0 };
    const BufferStorage storage_decoder2_ResidualBlock_1_body_conv_dw_bias = { InitializerBuffer };
    const Tensor1f< BufferStorage > decoder2_ResidualBlock_1_body_conv_dw_bias = {
        16, // logicalSize
        0, // threadGroupSliceStart
        16, // threadGroupSliceSize
        16, // storageSize
        4, // storageByteStrides
        0, // paddingBegin
        0, // paddingEnd
        6656, // threadGroupStorageByteOffset
        storage_decoder2_ResidualBlock_1_body_conv_dw_bias };
    const BufferStorage storage_hwnc__decoder2_ResidualBlock_1_body_conv_pw_expand_weight_quant_export_handler_QuantizeLinear_output_0 = { InitializerBuffer };
    const QuantizedTensor4f8_HWNC< BufferStorage > hwnc__decoder2_ResidualBlock_1_body_conv_pw_expand_weight_quant_export_handler_QuantizeLinear_output_0 = {
        uint4(1, 1, 16, 32), // logicalSize
        uint4(0, 0, 0, 0), // threadGroupSliceStart
        uint4(1, 1, 16, 32), // threadGroupSliceSize
        uint4(1, 1, 16, 32), // storageSize
        uint4(512, 512, 1, 16), // storageByteStrides
        uint4(0, 0, 0, 0), // paddingBegin
        uint4(0, 0, 0, 0), // paddingEnd
        114984, // threadGroupStorageByteOffset
        1.0, storage_hwnc__decoder2_ResidualBlock_1_body_conv_pw_expand_weight_quant_export_handler_QuantizeLinear_output_0 };
    const BufferStorage storage_decoder2_ResidualBlock_1_body_conv_pw_expand_bias = { InitializerBuffer };
    const Tensor1f< BufferStorage > decoder2_ResidualBlock_1_body_conv_pw_expand_bias = {
        32, // logicalSize
        0, // threadGroupSliceStart
        32, // threadGroupSliceSize
        32, // storageSize
        4, // storageByteStrides
        0, // paddingBegin
        0, // paddingEnd
        6720, // threadGroupStorageByteOffset
        storage_decoder2_ResidualBlock_1_body_conv_pw_expand_bias };
    const BufferStorage storage_hwnc__decoder2_ResidualBlock_1_body_conv_pw_contract_weight_quant_export_handler_QuantizeLinear_output_0 = { InitializerBuffer };
    const QuantizedTensor4f8_HWNC< BufferStorage > hwnc__decoder2_ResidualBlock_1_body_conv_pw_contract_weight_quant_export_handler_QuantizeLinear_output_0 = {
        uint4(1, 1, 32, 16), // logicalSize
        uint4(0, 0, 0, 0), // threadGroupSliceStart
        uint4(1, 1, 32, 16), // threadGroupSliceSize
        uint4(1, 1, 32, 16), // storageSize
        uint4(512, 512, 1, 32), // storageByteStrides
        uint4(0, 0, 0, 0), // paddingBegin
        uint4(0, 0, 0, 0), // paddingEnd
        115496, // threadGroupStorageByteOffset
        1.0, storage_hwnc__decoder2_ResidualBlock_1_body_conv_pw_contract_weight_quant_export_handler_QuantizeLinear_output_0 };
    const BufferStorage storage_decoder2_ResidualBlock_1_body_conv_pw_contract_bias = { InitializerBuffer };
    const Tensor1f< BufferStorage > decoder2_ResidualBlock_1_body_conv_pw_contract_bias = {
        16, // logicalSize
        0, // threadGroupSliceStart
        16, // threadGroupSliceSize
        16, // storageSize
        4, // storageByteStrides
        0, // paddingBegin
        0, // paddingEnd
        6848, // threadGroupStorageByteOffset
        storage_decoder2_ResidualBlock_1_body_conv_pw_contract_bias };
    // fused_quantized_NHWC_/decoder2/ResidualBlock_2/body/input_quantization/act_quant/export_handler/QuantizeLinear_output_0
    const uint3 logicalSize_slice_24 = uint3(3840, 2160, 16);
    const int3 groupStart_slice_24 = int3(0, 0, 0) + ml2c_groupId.xyz * int3(16, 8, 16);
    const uint3 groupSize_slice_24 = uint3(16, 8, 16);
    const uint3 storageSize_slice_24 = uint3(3842, 2162, 16);
    const uint3 tensorByteStrides_slice_24 = uint3(16, 61472, 1);
    const uint3 paddingBegin_slice_24 = uint3(1, 1, 0);
    const uint3 paddingEnd_slice_24 = uint3(1, 1, 0);
    const int threadGroupByteOffsetInTensor_slice_24 = dot(groupStart_slice_24, tensorByteStrides_slice_24);
    const float quantizationScale_slice_24 = 1.0;
    const RWBufferStorage storage_slice_24 = { ScratchBuffer };
    const QuantizedTensor3f8_NHWC<RWBufferStorage> slice_24 = { logicalSize_slice_24, groupStart_slice_24, groupSize_slice_24, storageSize_slice_24, tensorByteStrides_slice_24, paddingBegin_slice_24, paddingEnd_slice_24, threadGroupByteOffsetInTensor_slice_24 + 0, quantizationScale_slice_24, storage_slice_24 };
    // ConvNextBlock (16, 2160, 3840), (16, 16, 3, 3), (16,), (32, 16, 1, 1), (32,), (16, 32, 1, 1), (16,) -> (16, 2160, 3840)
    ComputeShaderParams computeShaderParams = {ml2c_numThreads, ml2c_groupId, ml2c_groupThreadId, ml2c_dispatchThreadId};
    ConvNextBlock(1.0, 1.0, 1.0, 1.0, fused_fused_quantized_NHWC__decoder2_ResidualBlock_1_body_input_quantization_act_quant_export_handler_QuantizeLinear_output_0, hwnc__decoder2_ResidualBlock_1_body_conv_dw_weight_quant_export_handler_QuantizeLinear_output_0, decoder2_ResidualBlock_1_body_conv_dw_bias, hwnc__decoder2_ResidualBlock_1_body_conv_pw_expand_weight_quant_export_handler_QuantizeLinear_output_0, decoder2_ResidualBlock_1_body_conv_pw_expand_bias, hwnc__decoder2_ResidualBlock_1_body_conv_pw_contract_weight_quant_export_handler_QuantizeLinear_output_0, decoder2_ResidualBlock_1_body_conv_pw_contract_bias, slice_24, computeShaderParams);
    
}
#endif // #ifdef MLSR_PASS_12
#ifdef MLSR_PASS_12_POST
#define ML2C_GROUPSHARED_SIZE 0

#include "ml2code_runtime/padding.hlsli"
#include "ml2code_runtime/tensor_float8.hlsli"

[numthreads(32, 1, 1)]
void fsr4_model_v07_fp8_no_scale_pass12_post(
    uint3 ml2c_dispatchThreadId : SV_DispatchThreadID,
    uint3 ml2c_groupId : SV_GroupID
)
{
    // fused_quantized_NHWC_/decoder2/ResidualBlock_2/body/input_quantization/act_quant/export_handler/QuantizeLinear_output_0
    const uint3 logicalSize_slice_25 = uint3(3840, 2160, 16);
    const int3 groupStart_slice_25 = int3(0, 0, 0) + ml2c_groupId.xyz * int3(32, 1, 16);
    const uint3 groupSize_slice_25 = uint3(32, 1, 16);
    const uint3 storageSize_slice_25 = uint3(3842, 2162, 16);
    const uint3 tensorByteStrides_slice_25 = uint3(16, 61472, 1);
    const uint3 paddingBegin_slice_25 = uint3(1, 1, 0);
    const uint3 paddingEnd_slice_25 = uint3(1, 1, 0);
    const int threadGroupByteOffsetInTensor_slice_25 = dot(groupStart_slice_25, tensorByteStrides_slice_25);
    const float quantizationScale_slice_25 = 1.0;
    const RWBufferStorage storage_slice_25 = { ScratchBuffer };
    const QuantizedTensor3f8_NHWC<RWBufferStorage> slice_25 = { logicalSize_slice_25, groupStart_slice_25, groupSize_slice_25, storageSize_slice_25, tensorByteStrides_slice_25, paddingBegin_slice_25, paddingEnd_slice_25, threadGroupByteOffsetInTensor_slice_25 + 0, quantizationScale_slice_25, storage_slice_25 };
    
    StoreDefaultConstBatchOperation < 16, QuantizedTensor3f8_NHWC<RWBufferStorage> > batchOp_0 = { DefaultDword };
    ResetPaddingSeparate(slice_25, ml2c_dispatchThreadId, true, true, batchOp_0, (1, 1));
}
#endif // #ifdef MLSR_PASS_12_POST


#ifdef MLSR_PASS_13
#define ML2C_GROUPSHARED_SIZE 0

#include "ml2code_runtime/operators/float16_NHWC/Fused/CNB_CT2D.hlsli"

[numthreads(32, 1, 1)]
void fsr4_model_v07_fp8_no_scale_pass13(
    uint3 ml2c_dispatchThreadId : SV_DispatchThreadID,
    uint3 ml2c_groupId : SV_GroupID,
    uint3 ml2c_groupThreadId : SV_GroupThreadID
)
{
    const uint3 ml2c_numThreads = uint3(32, 1, 1);
    
    const RWBufferStorage storage_fused_quantized_NHWC__decoder2_ResidualBlock_2_body_input_quantization_act_quant_export_handler_QuantizeLinear_output_0 = { ScratchBuffer };
    const QuantizedTensor3f8_NHWC< RWBufferStorage > fused_quantized_NHWC__decoder2_ResidualBlock_2_body_input_quantization_act_quant_export_handler_QuantizeLinear_output_0 = {
        uint3(3840, 2160, 16), // logicalSize
        uint3(0, 0, 0), // threadGroupSliceStart
        uint3(3840, 2160, 16), // threadGroupSliceSize
        uint3(3842, 2162, 16), // storageSize
        uint3(16, 61472, 1), // storageByteStrides
        uint3(1, 1, 0), // paddingBegin
        uint3(1, 1, 0), // paddingEnd
        0, // threadGroupStorageByteOffset
        1.0, storage_fused_quantized_NHWC__decoder2_ResidualBlock_2_body_input_quantization_act_quant_export_handler_QuantizeLinear_output_0 };
    
    const BufferStorage storage_hwnc__decoder2_ResidualBlock_2_body_conv_dw_weight_quant_export_handler_QuantizeLinear_output_0 = { InitializerBuffer };
    const QuantizedTensor4f8_HWNC< BufferStorage > hwnc__decoder2_ResidualBlock_2_body_conv_dw_weight_quant_export_handler_QuantizeLinear_output_0 = {
        uint4(3, 3, 16, 16), // logicalSize
        uint4(0, 0, 0, 0), // threadGroupSliceStart
        uint4(3, 3, 16, 16), // threadGroupSliceSize
        uint4(3, 3, 16, 16), // storageSize
        uint4(256, 768, 1, 16), // storageByteStrides
        uint4(0, 0, 0, 0), // paddingBegin
        uint4(0, 0, 0, 0), // paddingEnd
        116008, // threadGroupStorageByteOffset
        1.0, storage_hwnc__decoder2_ResidualBlock_2_body_conv_dw_weight_quant_export_handler_QuantizeLinear_output_0 };
    const BufferStorage storage_decoder2_ResidualBlock_2_body_conv_dw_bias = { InitializerBuffer };
    const Tensor1f< BufferStorage > decoder2_ResidualBlock_2_body_conv_dw_bias = {
        16, // logicalSize
        0, // threadGroupSliceStart
        16, // threadGroupSliceSize
        16, // storageSize
        4, // storageByteStrides
        0, // paddingBegin
        0, // paddingEnd
        6912, // threadGroupStorageByteOffset
        storage_decoder2_ResidualBlock_2_body_conv_dw_bias };
    const BufferStorage storage_hwnc__decoder2_ResidualBlock_2_body_conv_pw_expand_weight_quant_export_handler_QuantizeLinear_output_0 = { InitializerBuffer };
    const QuantizedTensor4f8_HWNC< BufferStorage > hwnc__decoder2_ResidualBlock_2_body_conv_pw_expand_weight_quant_export_handler_QuantizeLinear_output_0 = {
        uint4(1, 1, 16, 32), // logicalSize
        uint4(0, 0, 0, 0), // threadGroupSliceStart
        uint4(1, 1, 16, 32), // threadGroupSliceSize
        uint4(1, 1, 16, 32), // storageSize
        uint4(512, 512, 1, 16), // storageByteStrides
        uint4(0, 0, 0, 0), // paddingBegin
        uint4(0, 0, 0, 0), // paddingEnd
        118312, // threadGroupStorageByteOffset
        1.0, storage_hwnc__decoder2_ResidualBlock_2_body_conv_pw_expand_weight_quant_export_handler_QuantizeLinear_output_0 };
    const BufferStorage storage_decoder2_ResidualBlock_2_body_conv_pw_expand_bias = { InitializerBuffer };
    const Tensor1f< BufferStorage > decoder2_ResidualBlock_2_body_conv_pw_expand_bias = {
        32, // logicalSize
        0, // threadGroupSliceStart
        32, // threadGroupSliceSize
        32, // storageSize
        4, // storageByteStrides
        0, // paddingBegin
        0, // paddingEnd
        6976, // threadGroupStorageByteOffset
        storage_decoder2_ResidualBlock_2_body_conv_pw_expand_bias };
    const BufferStorage storage_hwnc__decoder2_ResidualBlock_2_body_conv_pw_contract_weight_quant_export_handler_QuantizeLinear_output_0 = { InitializerBuffer };
    const QuantizedTensor4f8_HWNC< BufferStorage > hwnc__decoder2_ResidualBlock_2_body_conv_pw_contract_weight_quant_export_handler_QuantizeLinear_output_0 = {
        uint4(1, 1, 32, 16), // logicalSize
        uint4(0, 0, 0, 0), // threadGroupSliceStart
        uint4(1, 1, 32, 16), // threadGroupSliceSize
        uint4(1, 1, 32, 16), // storageSize
        uint4(512, 512, 1, 32), // storageByteStrides
        uint4(0, 0, 0, 0), // paddingBegin
        uint4(0, 0, 0, 0), // paddingEnd
        118824, // threadGroupStorageByteOffset
        1.0, storage_hwnc__decoder2_ResidualBlock_2_body_conv_pw_contract_weight_quant_export_handler_QuantizeLinear_output_0 };
    const BufferStorage storage_decoder2_ResidualBlock_2_body_conv_pw_contract_bias = { InitializerBuffer };
    const Tensor1f< BufferStorage > decoder2_ResidualBlock_2_body_conv_pw_contract_bias = {
        16, // logicalSize
        0, // threadGroupSliceStart
        16, // threadGroupSliceSize
        16, // storageSize
        4, // storageByteStrides
        0, // paddingBegin
        0, // paddingEnd
        7104, // threadGroupStorageByteOffset
        storage_decoder2_ResidualBlock_2_body_conv_pw_contract_bias };
    const BufferStorage storage_hwcn__decoder2_UpscaleConvTranspose2x2_upscale_conv_weight_quant_export_handler_QuantizeLinear_output_0 = { InitializerBuffer };
    const QuantizedTensor4f8_HWCN< BufferStorage > hwcn__decoder2_UpscaleConvTranspose2x2_upscale_conv_weight_quant_export_handler_QuantizeLinear_output_0 = {
        uint4(2, 2, 8, 16), // logicalSize
        uint4(0, 0, 0, 0), // threadGroupSliceStart
        uint4(2, 2, 8, 16), // threadGroupSliceSize
        uint4(2, 2, 8, 16), // storageSize
        uint4(128, 256, 16, 1), // storageByteStrides
        uint4(0, 0, 0, 0), // paddingBegin
        uint4(0, 0, 0, 0), // paddingEnd
        129576, // threadGroupStorageByteOffset
        1.0, storage_hwcn__decoder2_UpscaleConvTranspose2x2_upscale_conv_weight_quant_export_handler_QuantizeLinear_output_0 };
    const BufferStorage storage_decoder2_UpscaleConvTranspose2x2_upscale_conv_bias = { InitializerBuffer };
    const Tensor1f< BufferStorage > decoder2_UpscaleConvTranspose2x2_upscale_conv_bias = {
        8, // logicalSize
        0, // threadGroupSliceStart
        8, // threadGroupSliceSize
        8, // storageSize
        4, // storageByteStrides
        0, // paddingBegin
        0, // paddingEnd
        7168, // threadGroupStorageByteOffset
        storage_decoder2_UpscaleConvTranspose2x2_upscale_conv_bias };
    // fused_quantized_NHWC_output
    const uint3 logicalSize_fused_quantized_NHWC_output = uint3(7680, 4320, 8);
    const int3 groupStart_fused_quantized_NHWC_output = int3(0, 0, 0) + ml2c_groupId.xyz * int3(64, 2, 8);
    const uint3 groupSize_fused_quantized_NHWC_output = uint3(64, 2, 8);
    const uint3 storageSize_fused_quantized_NHWC_output = uint3(7680, 4320, 8);
    const uint3 tensorByteStrides_fused_quantized_NHWC_output = uint3(16, 122880, 2);
    const uint3 paddingBegin_fused_quantized_NHWC_output = uint3(0, 0, 0);
    const uint3 paddingEnd_fused_quantized_NHWC_output = uint3(0, 0, 0);
    const int threadGroupByteOffsetInTensor_fused_quantized_NHWC_output = dot(groupStart_fused_quantized_NHWC_output, tensorByteStrides_fused_quantized_NHWC_output);
    const RWBufferStorage storage_fused_quantized_NHWC_output = { buffer_fused_quantized_NHWC_output };
    const Tensor3h_NHWC<RWBufferStorage> fused_quantized_NHWC_output = { logicalSize_fused_quantized_NHWC_output, groupStart_fused_quantized_NHWC_output, groupSize_fused_quantized_NHWC_output, storageSize_fused_quantized_NHWC_output, tensorByteStrides_fused_quantized_NHWC_output, paddingBegin_fused_quantized_NHWC_output, paddingEnd_fused_quantized_NHWC_output, threadGroupByteOffsetInTensor_fused_quantized_NHWC_output + 0, storage_fused_quantized_NHWC_output };
    // FusedConvNextBlock_quantized_/decoder2/UpscaleConvTranspose2x2/upscale_conv/ConvTranspose (16, 2160, 3840), (16, 16, 3, 3), (16,), (32, 16, 1, 1), (32,), (16, 32, 1, 1), (16,), (16, 8, 2, 2), (8,) -> (8, 4320, 7680)
    ComputeShaderParams computeShaderParams = {ml2c_numThreads, ml2c_groupId, ml2c_groupThreadId, ml2c_dispatchThreadId};
    CNB_CT2D<8>(1.0, 1.0, 1.0, 1.0, 1.0, fused_quantized_NHWC__decoder2_ResidualBlock_2_body_input_quantization_act_quant_export_handler_QuantizeLinear_output_0, hwnc__decoder2_ResidualBlock_2_body_conv_dw_weight_quant_export_handler_QuantizeLinear_output_0, decoder2_ResidualBlock_2_body_conv_dw_bias, hwnc__decoder2_ResidualBlock_2_body_conv_pw_expand_weight_quant_export_handler_QuantizeLinear_output_0, decoder2_ResidualBlock_2_body_conv_pw_expand_bias, hwnc__decoder2_ResidualBlock_2_body_conv_pw_contract_weight_quant_export_handler_QuantizeLinear_output_0, decoder2_ResidualBlock_2_body_conv_pw_contract_bias, hwcn__decoder2_UpscaleConvTranspose2x2_upscale_conv_weight_quant_export_handler_QuantizeLinear_output_0, decoder2_UpscaleConvTranspose2x2_upscale_conv_bias, fused_quantized_NHWC_output, computeShaderParams);
    
}
#endif // #ifdef MLSR_PASS_13


